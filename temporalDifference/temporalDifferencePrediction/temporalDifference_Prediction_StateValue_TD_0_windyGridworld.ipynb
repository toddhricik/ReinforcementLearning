{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Temporal Difference Methods For Prediction of State Value Function: Using On Policy One-Step TD(0) for Windy Gridworld</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Difference methods are another class of tabular solutions which can be used to estimate the state value function $V(S_{t})$ described by the Bellman eauations.[[4]](#References) This notebook contains functions that estimate $V(S_{t})\\approx v_{\\pi}(s_{t})$ using the One Step TD(0) algorithm for the Windy Gridworld problem described in [[5]](#References)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Environment Windy Gridworld</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Windy Gridworld environment is composed of a class containing the maze itself, the rules for wind effects within the maze and helper functions to compute the rewards associated with any of the available actions. See the [Windy Gridworld environment class notebook](envWindyGridworld.ipynb) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: Maze</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maze is represented as a 20x20 numpy array of integers where each elelment of the 2D array represents a position in the maze and the value of each element signifies whether the position can be occupied by the agent (i.e. position is not a wall), and whether the position is \"windy\" in one in any of the possible directions. Landing on a \"windy\" position in the maze results in the agent being blown into the nearest wall that lies down wind from the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large maze[i,j]\n",
    "    = \n",
    "    \\begin{cases}\n",
    "    -1\\quad\\,\\,\\,\\,\\,\\,\\,\\,\\normalsize\\text{position is a wall} \\\\\n",
    "    0\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind is not blowing } \\\\\n",
    "    1\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows N } \\\\\n",
    "    2\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows E} \\\\\n",
    "    3\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows S} \\\\\n",
    "    4\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows W} \n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put a picture of the Windy Gridworld maze here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: State Space</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each white square in the maze represnts a state which is a random variable denoted as $S$.\n",
    "Each observed state $s\\in S$ is defined as an ordered pair $(x, y)$ which represents the position of the agent on the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large S\\coloneqq\\{(x, y): (x\\in\\mathbb{Z}) \\cap (0\\le x<n) \\bigcap (y\\in\\mathbb{Z}) \\cap (0\\le y<m)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action is a random variable which we denote with the symbol $A$ and there are four actions available to the agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large A\\coloneqq\\{Up, Down, Left, Right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Environment Windy Gridworld: Episodes and Time Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gridworld, an episode consists of a discrete sequence of time steps that occur from the time when the agent first begins, at the \"Start\" square on our grid, to the time at which our agent finally reaches the \"Finish\" square on the grid.\n",
    "    \n",
    "The time step is denoted by the variable $t$ and is set to zero at the beginning of the first episode. At this time our agent is located in the \"Start\" square on the grid.\n",
    "\n",
    "The time step variable $t$ is then incremented by one after each action is take by our agent until it reaches the \"Finish\" sqaure on the grid.\n",
    "\n",
    "The variable $T$ is used to represent the value of the time step $t$ upon which our agent reaches the \"Finish\" square on the grid and the episode ends.\n",
    "\n",
    "If another episode is carried out, the agent returns to the \"Start\" square and the time step variable $t$ is set to $T+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: Rewards and Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Gridworld: Rewards</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step the reward of an action taken is either -1 or 0 depending upon whether the next state is a finish line or non-finish line square on the grid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\large R_t\n",
    "    =\n",
    "    \\begin{cases}\n",
    "    0\\quad\\quad \\Large s_{t+1}\\normalsize =\\text{ finish line square on the grid } \\\\\n",
    "    -1\\quad\\, \\Large s_{t+1}\\normalsize\\neq\\text{ finish line square on the grid}\n",
    "    \\end{cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>State Value Function</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bellman eqautions, the state value function at time $t$, $v_{\\pi}(s_{t})$, within an episode represents the expected return when in state $s_{t}\\in S$ according to policy $\\pi$. [\\[1\\]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad v_\\pi(s_t)\\coloneqq\\mathbb{E} [G_t | S_t=s_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma G_{t+1} | S_t=s_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\,\\,|\\,\\,S_{t}=s_{t}]$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Monte Carlo state value prediction methods, TD methods update the estimate of the state value function at each time-step within an episode (instead of updating the estimate at the end of each episode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V(S_{t})\\leftarrow V(S_{t})+\\alpha[R_{t+1}+\\gamma V(S_{t+1})-V(S_{t})])]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent's policy is denoted as $\\\\pi$ and is used to represent the conditional probability mass function $f_{A|S}$ of actions over the states:<br>\n",
    "    $$\\large \\pi\\coloneqq f_{A|S}$$\n",
    "    At any given timestep $t$, our agent must choose an action $a\\in A$ to move from its current state $s\\in S$ to its next state $s'\\in S$.\n",
    "    We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "    $$\\large \\pi(a|s)=f_{A|S}(a|s)=P(A=a|S=s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy used in Windy Gridworld is represented by an $mxnx|S|$ numpy array pi where each element represents the probability of taking action $a\\in A$ while in the state $s=(i,j)$\n",
    "   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example a random policy, example_pi, for taking actions $A$ with equal probability in a $2x2$ maze would have the shape (2,2,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pi = np.zeros((2,2,4),dtype=np.float32)\n",
    "example_pi[:] = 0.25\n",
    "print(example_pi)\n",
    "example_pi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>State Value Function</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $mxn$ matrix $V$ is used to store the values of each state $v_\\pi(s)$ on the grid where, according to the Bellman equations, the state value represents the expected return of the current state while folowing policy $\\pi.$[[4]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example the 2D numpy array V for a maze of shape $2x2$ represents the state value $v_\\pi(s)$ for each $s\\in S$ and would have the shape (2,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1345076  0.83796436]\n",
      " [0.98025865 0.94497563]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_V = np.random.sample((2,2))\n",
    "print(example_V)\n",
    "example_V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TD(0) Prediction State Value Algorithm</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psuedocode for the TD(0) Prediction State Value algorithm which estimates the state value function $V(s_t)$ is found below and as described in Barto et. al. [[1]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\text{TD(0) Prediction State Value Algorithm}$<br>\n",
    "\n",
    "$\\large\\quad\\quad\\text{Inputs:}\\,\\,\\text{target policy\\,\\,}\\pi,\\,\\,\\,environment,\\,\\,stepSize\\,\\alpha,\\,\\,\\text{discount factor }\\gamma,\\,\\,numEpisodes$<br>\n",
    "$\\large\\quad\\quad\\text{ 1.}\\quad V(terminal)\\leftarrow0,\\,V(s)\\leftarrow\\text{Randomly chosen state value for all non-terminal states }$<br>\n",
    "$\\large\\quad\\quad\\text{ 2.}\\quad\\text{Loop forever (for each episode)}:$<br>\n",
    "$\\large\\quad\\quad\\text{ 3.}\\quad\\quad\\text{Initialize }S(0)=s$<br>\n",
    "$\\large\\quad\\quad\\text{ 4.}\\quad\\quad\\text{Loop for each step of episode}:$<br>\n",
    "$\\large\\quad\\quad\\text{ 5.}\\quad\\quad\\quad A\\leftarrow\\text{ action given by policy }\\pi\\text{ for }S(0)$<br>\n",
    "$\\large\\quad\\quad\\text{ 6.}\\quad\\quad\\quad\\text{Take action }A\\text{, observe }R\\text{ and }S'$<br>\n",
    "$\\large\\quad\\quad\\text{ 7.}\\quad\\quad\\quad V(S)\\leftarrow\\,V(S)\\,+\\alpha[R+\\gamma V(S')\\,-\\,V(S)]$<br>\n",
    "$\\large\\quad\\quad\\text{ 8.}\\quad\\quad\\quad S\\leftarrow S'$<br>\n",
    "$\\large\\quad\\quad\\text{ 9.}\\quad\\quad\\quad\\text{until }S\\text{ is terminal}$<br>\n",
    "$\\large\\quad\\quad\\text{10.}\\quad\\quad\\text{Return }V(S)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple one-step TD method is called TD(0), which is a special case of the bootstrapping method called TD($\\gamma$), and combines the sampling strategies of Monte Carlo methods with the bootstrapping used in Dynamic Programming methods. [[1]](#References)[[2]](#References) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 120.\n",
    "2. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, pp. 142-145.\n",
    "3. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, pp. 287-301.\n",
    "4. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 58.\n",
    "5. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 130.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_python_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
