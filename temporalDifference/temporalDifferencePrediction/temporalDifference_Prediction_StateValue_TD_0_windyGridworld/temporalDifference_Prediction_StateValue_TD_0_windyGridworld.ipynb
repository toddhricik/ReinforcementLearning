{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Temporal Difference Methods For Prediction of State Value Function: Using On Policy One-Step TD(0) for Windy Gridworld</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Difference methods are another class of tabular solutions which can be used to estimate the state value function $V(S_{t})$ described by the Bellman eauations.[[4]](#References) This notebook contains functions that estimate $V(S_{t})\\approx v_{\\pi}(s_{t})$ using the One Step TD(0) algorithm for the Windy Gridworld problem described in [[5]](#References)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will require the following Python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpimg\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import import_ipynb\n",
    "import ipynb.fs.full.envWindyGridworld as wgw\n",
    "import ipynb.fs.full.targetPolicyWindyGridworld as tPolicy\n",
    "import ipynb.fs.full.returnsStateWindyGridworld as returnsStateWindyGridworld\n",
    "import ipynb.fs.full.stateValueWindyGridworld as stateValueWindyGridworld\n",
    "\n",
    "\n",
    "import ipynb.fs.full.helpers as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Environment Windy Gridworld</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put a picture of the Windy Gridworld maze here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Windy Gridworld environment is composed of a class containing the maze itself, the rules for wind effects within the maze and helper functions to compute the rewards associated with any of the available actions. See the [Windy Gridworld environment class notebook](envWindyGridworld.ipynb) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: Maze</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maze is represented as a 20x20 numpy array of integers where each elelment of the 2D array represents a position in the maze and the value of each element signifies whether the position can be occupied by the agent (i.e. position is not a wall), and whether the position is \"windy\" in one in any of the possible directions. Landing on a \"windy\" position in the maze results in the agent being blown into the nearest wall that lies down wind from the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large maze[i,j]\n",
    "    = \n",
    "    \\begin{cases}\n",
    "    -1\\quad\\,\\,\\,\\,\\,\\,\\,\\,\\normalsize\\text{position is a wall} \\\\\n",
    "    0\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind is not blowing } \\\\\n",
    "    1\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows N } \\\\\n",
    "    2\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows E} \\\\\n",
    "    3\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows S} \\\\\n",
    "    4\\quad\\quad \\,\\,\\,\\,\\,\\,\\normalsize\\text{position is not a wall and wind blows W} \n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predifined maze has been saved as a 2D numpy array. Let's load the 2D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.load('./maze.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -1,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,\n",
       "         0, -1,  0,  0],\n",
       "       [ 0, -1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0, -1,  0,  0],\n",
       "       [ 0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         0, -1,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0, -1,  0,  0],\n",
       "       [ 0,  0, -1,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0, -1,  0,  0,  3, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "         0,  0,  0, -1],\n",
       "       [ 0,  0, -1,  0,  0,  3, -1,  0,  0, -1, -1, -1,  0,  0,  0, -1,\n",
       "        -1, -1, -1,  0],\n",
       "       [ 0,  0, -1,  0,  0,  3, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0, -1,  0,  0,  3,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [-1, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0, -1,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [-1, -1,  0,  0,  0,  0, -1,  0,  0, -1,  0,  0,  0, -1,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [-1, -1,  0,  0,  0,  0, -1,  0,  0, -1,  0, -1,  0, -1,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,\n",
       "        -1, -1, -1, -1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0, -1, -1, -1, -1, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the positions (4, 5), (5, 5), (6, 5), (7, 5), (8, 5) and (9, 5) are all \"windy\" and blow in the southern direction since each of their states is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: State Space</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each white square in the maze represnts a state which is a random variable denoted as $S$.\n",
    "Each observed state $s\\in S$ is defined as an ordered pair $(x, y)$ which represents the position of the agent on the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large S\\coloneqq\\{(x, y): (x\\in\\mathbb{Z}) \\cap (0\\le x<n) \\bigcap (y\\in\\mathbb{Z}) \\cap (0\\le y<m)\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action is a random variable which we denote with the symbol $A$ and there are four actions available to the agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large A\\coloneqq\\{Up, Down, Left, Right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Environment Windy Gridworld: Episodes and Time Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gridworld, an episode consists of a discrete sequence of time steps that occur from the time when the agent first begins, at the \"Start\" square on our grid, to the time at which our agent finally reaches the \"Finish\" square on the grid.\n",
    "    \n",
    "The time step is denoted by the variable $t$ and is set to zero at the beginning of the first episode. At this time our agent is located in the \"Start\" square on the grid.\n",
    "\n",
    "The time step variable $t$ is then incremented by one after each action is take by our agent until it reaches the \"Finish\" sqaure on the grid.\n",
    "\n",
    "The variable $T$ is used to represent the value of the time step $t$ upon which our agent reaches the \"Finish\" square on the grid and the episode ends.\n",
    "\n",
    "If another episode is carried out, the agent returns to the \"Start\" square and the time step variable $t$ is set to $T+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Windy Gridworld: Rewards and Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Gridworld: Rewards</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step the reward of an action taken is either -1 or 0 depending upon whether the next state is a finish line or non-finish line square on the grid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\large R_t\n",
    "    =\n",
    "    \\begin{cases}\n",
    "    0\\quad\\quad \\Large s_{t+1}\\normalsize =\\text{ finish line square on the grid } \\\\\n",
    "    -1\\quad\\, \\Large s_{t+1}\\normalsize\\neq\\text{ finish line square on the grid}\n",
    "    \\end{cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>State Value Function</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bellman eqautions, the state value function at time $t$, $v_{\\pi}(s_{t})$, within an episode represents the expected return when in state $s_{t}\\in S$ according to policy $\\pi$. [\\[1\\]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad v_\\pi(s_t)\\coloneqq\\mathbb{E} [G_t | S_t=s_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma G_{t+1} | S_t=s_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\,\\,|\\,\\,S_{t}=s_{t}]$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Monte Carlo state value prediction methods, TD methods update the estimate of the state value function at each time-step within an episode (instead of updating the estimate at the end of each episode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large V(S_{t})\\leftarrow V(S_{t})+\\alpha[R_{t+1}+\\gamma V(S_{t+1})-V(S_{t})])]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $mxn$ matrix $V$ is used to store the values of each state $v_\\pi(s)$ on the grid where, according to the Bellman equations, the state value represents the expected return of the current state while folowing policy $\\pi.$[[4]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent's policy is denoted as $\\pi$ and is used to represent the conditional probability mass function $f_{A|S}$ of actions over the states:<br>\n",
    "    $$\\large \\pi\\coloneqq f_{A|S}$$\n",
    "    At any given timestep $t$, our agent must choose an action $a\\in A$ to move from its current state $s\\in S$ to its next state $s'\\in S$.\n",
    "    We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "    $$\\large \\pi(a|s)=f_{A|S}(a|s)=P(A=a|S=s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy used in Windy Gridworld is represented by an $mxnx|S|$ numpy array pi where each element represents the probability of taking action $a\\in A$ while in the state $s=(i,j)$\n",
    "   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example a random policy, example_pi, for taking actions $A$ with equal probability in a $2x2$ maze would have the shape (2,2,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]\n",
      "\n",
      " [[0.25 0.25 0.25 0.25]\n",
      "  [0.25 0.25 0.25 0.25]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pi = np.zeros((2,2,4),dtype=np.float32)\n",
    "example_pi[:] = 0.25\n",
    "print(example_pi)\n",
    "example_pi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del example_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>State Value Function</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example the 2D numpy array V for a maze of shape $2x2$ represents the state value $v_\\pi(s)$ for each $s\\in S$ and would have the shape (2,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1345076  0.83796436]\n",
      " [0.98025865 0.94497563]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_V = np.random.sample((2,2))\n",
    "print(example_V)\n",
    "example_V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del example_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TD(0) Prediction State Value Algorithm</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psuedocode for the TD(0) Prediction State Value algorithm which estimates the state value function $V(s_t)$ is found below and as described in Barto et. al. [[1]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\text{TD(0) Prediction State Value Algorithm}$<br>\n",
    "\n",
    "$\\large\\quad\\quad\\text{Inputs:}\\,\\,\\text{target policy\\,\\,}\\pi,\\,\\,\\,\\text{environment},\\,\\,\\text{stepSize}\\,\\alpha,\\,\\,\\text{discount factor }\\gamma,\\,\\,\\text{tolerance },\\,\\,\\,\\text{numEpisodes }$<br>\n",
    "$\\large\\quad\\quad\\text{ 1.}\\quad V(terminal)\\leftarrow0,\\,V(s)\\leftarrow\\text{Randomly chosen state value for all non-terminal states }$<br>\n",
    "$\\large\\quad\\quad\\text{ 2.}\\quad\\text{Loop forever (for each episode)}:$<br>\n",
    "$\\large\\quad\\quad\\text{ 3.}\\quad\\quad\\text{Initialize }S(0)=s$<br>\n",
    "$\\large\\quad\\quad\\text{ 4.}\\quad\\quad\\text{Loop for each step of episode}:$<br>\n",
    "$\\large\\quad\\quad\\text{ 5.}\\quad\\quad\\quad A\\leftarrow\\text{ action given by policy }\\pi\\text{ for }S(0)$<br>\n",
    "$\\large\\quad\\quad\\text{ 6.}\\quad\\quad\\quad\\text{Take action }A\\text{, observe }R\\text{ and }S_{t+1}$<br>\n",
    "$\\large\\quad\\quad\\text{ 7.}\\quad\\quad\\quad V(S_{t})\\leftarrow\\,V(S_{t})\\,+\\alpha[R+\\gamma V(S_{t+1})\\,-\\,V(S_{t})]$<br>\n",
    "$\\large\\quad\\quad\\text{ 8.}\\quad\\quad\\quad S\\leftarrow S_{t+1}$<br>\n",
    "$\\large\\quad\\quad\\text{ 9.}\\quad\\quad\\quad\\text{until }S\\text{ is terminal}$<br>\n",
    "$\\large\\quad\\quad\\text{10.}\\quad\\quad\\text{Return }V$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple one-step TD method is called TD(0), which is a special case of the bootstrapping method called TD($\\gamma$), and combines the sampling strategies of Monte Carlo methods with the bootstrapping used in Dynamic Programming methods. [[1]](#References)[[2]](#References) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TemporalDifferencePrediciton_StateValue_TD_0_WindyGridWorld(startPosition, targetPolicy, environment, stepSize, discountFactor, tolerance, numEpisodes):\n",
    "    # 1. Initialize V(terminal) to zero and all other V(s) to random state value\n",
    "    V = np.random.sample((environment.Maze.shape[0], environment.Maze.shape[1]))\n",
    "    # 1. Set the state value of walls within the maze to -1, and set the finish position state value to 0\n",
    "    for i in range(environment.Maze.shape[0]):\n",
    "        for j in range(environment.Maze.shape[1]):\n",
    "            if (environment.Maze[i,j] == -1):\n",
    "                V[i,j] = (float)('-inf')\n",
    "                continue\n",
    "            if (i == environment.finish_position[0] and j == environment.finish_position[1]):\n",
    "                V[i,j] = 0\n",
    "    # 2. Loop forever or for each episode\n",
    "    for _ in range(numEpisodes):\n",
    "        # 3. Intitialize the temporary placeholder S with \n",
    "        S = startPosition        \n",
    "        # 4. Loop over each step in the episode\n",
    "        isDone = False\n",
    "        while( not(isDone) ):\n",
    "              # 5. A is set to the action chosen for the current state given the policy\n",
    "              A = {}\n",
    "              # 6. Take action a and observe R and S_{t+1}\n",
    "\n",
    "              # 7. Update the estimate of V(S_{t})\n",
    "              V = {}\n",
    "              # 8. Set S to S_{t+1}\n",
    "              S = {}\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perform TD(0) for prediction of state value function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the step size alpha for updating the estimate of the state value function\n",
    "alpha = 0.1\n",
    "# Set the discount factor gamma for rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Set a tolerance for comparison of updated state values for state value estimation\n",
    "tolerance = 0.0000000001\n",
    "\n",
    "# Select a square in the Windy Gridworld maze to act as the starting line square\n",
    "start_position = (19, 0)\n",
    "\n",
    "# Select a square in the Windy Gridworld maze to act as the finish line square\n",
    "finish_position = (0,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set teh number of runs\n",
    "numRuns = 1\n",
    "# episodes [ [1, 10, 50, 100, 200, 500, 1000, 3000, 5000, 10000]\n",
    "episodes = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An environment can be instantiated by simply choosing a finish position and then calling the envGridworld constructor which takes a path to a numpy array representing the maze and the selected finishing square position as a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the TD(0) algorithm on the Windy Gridworld environmnet object using the parameters set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "for i in range(len(episodes)):\n",
    "    # Run an experiment numRuns times\n",
    "    for runNum in range(numRuns):\n",
    "        # Set the environment\n",
    "        env = wgw.envWindyGridworld(path_to_maze=\"./maze.npy\", start_position = start_position, finish_position = finish_position)\n",
    "        # Open a pre-trained target policy that was serialized to json\n",
    "        with open('policies/dumbPolicy.pickle', 'rb') as handle:\n",
    "            pi = pickle.load(handle)\n",
    "        # Instatiate and initialize a new state value function V(S_t)\n",
    "            V_s = stateValue.stateValueWindyGridworld(staetSpace=stateSpaceWindyGridworld, actionSpace=actionSpaceWindyGridworld)\n",
    "        # Instantiate and uinitialize a new returns runction R(S_t) with empty lists\n",
    "            R_s = returnsS\n",
    "        # Run the TD(0) prediciton algorithm\n",
    "        v_star_estimate, path_to_state_value_fig =  TemporalDifferencePrediciton_StateValue_TD_0_WindyGridWorld(targetPolicy=pi, environment=env, V_s=V_s, R_s=R_s, stepSize=alpha, discountFactor=gamma, numEpisodes=numEpisodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TD(0) Estimation of State Value Function Results for Windy Gridworld</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting estimates for the state value function for all states in the Windy Gridworld environment are provided below where each position in the maze contains:\n",
    "1. the direction of the wind (if any)\n",
    "2. the estimated state value $v_{\\pi}$ of each position on the grid while following policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=path_to_state_value_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 120.\n",
    "2. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, pp. 142-145.\n",
    "3. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, pp. 287-301.\n",
    "4. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 58.\n",
    "5. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 130.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_python_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
