{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MonteCarloPrediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_FirstVisit_Blackjack\"></a>\n",
    "<h1>Monte Carlo Prediction of State Value Function: Using Off Policy Ordinary Importance Sampling First Visit for Blackjack</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo prediction algorithms, such as Off Policy Ordinary Importance Sampling First Visit, can be used to estimate the state value and action-value functions, $V(S)$ and $Q(S, A)$, described by the Bellman equations.[\\[1\\]](#references)  This notebook contains a funtions that compute estimates for $V(S_{t})\\approx v_{\\pi}(s_{t})$ and $Q(S_{t}, A_{t})\\approx q_{\\pi}(s_{t}, a_{t})$ using the [Off Policy Orinary Importance Sampling First Visit](#MonteCarloPrediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_FirstVisit_Blackjack) algorithm for the Blackjack problem described in Example 5.3 of the text by Sutton and Barto. [\\[2\\]](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will require the following python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image\n",
    "import import_ipynb\n",
    "import ipynb.fs.full.policyBlackjack as tPolicy\n",
    "import ipynb.fs.full.targetPolicyBlackjack as tPolicy\n",
    "import ipynb.fs.full.behaviorPolicyBlackjack as bPolicy\n",
    "import ipynb.fs.full.returnsStateBlackjack as returnsS\n",
    "import ipynb.fs.full.stateActionValueBlackjack as actionValue\n",
    "import ipynb.fs.full.stateValueBlackjack as stateValue\n",
    "import ipynb.fs.full.helpers_MonteCarloPrediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_FirstVisit_Blackjack as h\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Environment: Blackjack</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each game (i.e. episode) begins as soon as the first four cards are dealt (two face up agent cards, one dealer card face-down, and one dealer card face-up). Each episode proceeds and ends as described by the Blackjack environment within the open source reinforcement learning API, Gymnasium,  provided by Farama Foundation. [\\[2\\]](#references) A few of the key components of the Blackjack environment are described below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: State Space</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state of a Blackjack game, represented by the random variable $S$,  consists of a 3-tuple containing 1.) the player's current score via summing the values of the cards in the player's hand, 2.) the value of the card shown by the dealer and 3.) whether the player is holding a usable ace. [\\[2\\]](#references)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large S\\coloneqq\\{(playerSum,\\,\\,dealerShowing,\\,\\,usableAce)\\in\\mathbb{Z}^{3}: (0\\le playerSum \\le 32)\\cap(0\\le dealerShowing \\le 11)\\cap (0\\le usableAce\\le 2)\\}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The state space (often refered to as the \"observation space\" within the Gymnasium documentation) covers instances of states that are not reachable in any Blackjack game. More specifically, player sums of 0 or 1 and dealer showing values of 0 are not possible given the constraints of the environment (i.e. the game of Blackjack). This was a design choice made by the Gym API developers (while still being maintained by OpenAI) in order to faciliate \"easier indexing for table based algorithms\".  processing of numerical results within the environment API. [\\[3\\]](#references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state spaces for player sum, dealer showing card, and usable ace\n",
    "# Note: Gymnasium's observation space is a Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
    "# which contains observations that are not possible. There are very few of them so continue\n",
    "# to set Q_s_a values according to the observation space assumed by Gymnasium\n",
    "# Note: the observation space contains unreachable states. This was a choice made by\n",
    "# the OpenAI developers to facilitate easy indexing. See https://github.com/opoenai/gym/issues/1410\n",
    "# for more information.\n",
    "stateSpacePlayerSum = [i for i in range(32)]\n",
    "stateSpaceDealerShows = [i for i in range(1, 11)]\n",
    "stateSpaceUsableAce = [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these enumerations of state values later on. For now let's talk about the Action Space of the Blackjack environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: Action Space</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent may choose to either 'hit' or 'stick', at each timepoint $t$ during an episode.   Each action taken by the agent (i.e. to either 'stick' or 'hit') is represented by an instance of the random variable $A$ which is sampled from from the Bernoulli distribution.. and there are two actions ('Hit' or 'Stick') available to the agent. [\\[2\\]](#references)<br>\n",
    "$$\\large A\\coloneqq\\{a\\in\\{0, 1\\} : a=0\\text{ when player 'sticks'} \\cap a=1\\text{ when player 'hits'}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create some lists to help us build the state and action spaces later on within the exploring starts algorithm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the action space (0='Stick', 1='Hit') according to the Gymnasium documentation\n",
    "actionSpace = [i for i in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this enumeration of actions within the Monte Carlo Prediction Ordinary Off Policy Importance Sampling First Visit algorithm later below. For now let's talk about how the Blackjack environment manages episdoes and time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: Episodes and Time Steps</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Blackjack environment within the Gymnasium API, an episode consists of what is commonly called a game in a real-life Blackjack scenario. Each episode consists of $T$ timesteps {$t_{0}$, $t_{1}$, $\\dots$,$t_{T-1}$}. The initial state $s_{t=0}$ is provided by the Blackjack environment each time the environment is reset. The Monte Carlo Prediction State Value Function Ordinary Importance Sampling First Visit Blackjack algorithm randomly assigns an action $a_{t=0}$ which then form the state-action pair ($S_{t}=s_{t}$, $A_{t}=a_{t}$). Each episode has the following form:<br>\n",
    "$$\\large\\text{Episode } \\coloneqq \\{ S_{0},\\,\\,A_{0},\\,\\,R_{1},\\,\\,S_{1},\\,\\,A_{1},\\,\\,R_{2},\\,\\,\\dots,\\,\\,S_{T-1},\\,\\,A_{T-1},\\,\\,R_{T}\\}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: Rewards and Returns</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward calculations are carried out at the end of each episode when using the Monte Carlo Prediction Off Policy Ordinary Importance Sampling First Visit algorithm.  Each game (i.e. episode) concludes with assignment of a reward to the random variable $R_{T}$ according to the following five outcomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large R_T\n",
    "= \n",
    "\\begin{cases}\n",
    "-1\\quad\\text{dealer wins} \\\\\n",
    "0\\quad\\quad\\text{draw} \\\\\n",
    "1\\quad\\quad\\text{agent wins with non natural} \\\\\n",
    "1\\quad\\quad\\text{agent wins on natural(if natural is set to False)} \\\\\n",
    "1.5\\quad\\text{agent wins on natural (if natural is set to True)}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that an episode has ended and the reward, $R_{T}$, has been returned from the Blackjack environment, we set the reward at time step $t$ of the episode to $R_{T}$<br>\n",
    "$$\\large R_{t} = R_{T}\\,\\,\\forall\\,\\,0\\le\\,\\,t\\,\\,\\lt\\,\\,T$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC Monte Carlo Prediction Off Policy Ordinary Importance Sampling First Visit algorithm will use a returns function, $Returns(s_{t}, a_{t})$, to help us keep track of which states accumulate rewards $R_{T}$ at the end of each episode.<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large Returns(s_{t}) \\leftarrow\\text{list of accumulated returns }G\\text{ that resulted from being in state }s_{t}\\text{ over all episodes of a single run.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary R_s is a member of the [returnsStateBlackjack class](#returnsStateBlackjack.ipynb) and uses the key={((playerSum, dealerShows, usableAce)} : value={$Returns(s)$} and will be used later within the Monte Carlo Prediction State Value Off Policy Ordinary Importance Sampling First Visit algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([((0, 1, True), []), ((0, 1, False), []), ((0, 2, True), []), ((0, 2, False), []), ((0, 3, True), []), ((0, 3, False), []), ((0, 4, True), []), ((0, 4, False), []), ((0, 5, True), []), ((0, 5, False), []), ((0, 6, True), []), ((0, 6, False), []), ((0, 7, True), []), ((0, 7, False), []), ((0, 8, True), []), ((0, 8, False), []), ((0, 9, True), []), ((0, 9, False), []), ((0, 10, True), []), ((0, 10, False), []), ((1, 1, True), []), ((1, 1, False), []), ((1, 2, True), []), ((1, 2, False), []), ((1, 3, True), []), ((1, 3, False), []), ((1, 4, True), []), ((1, 4, False), []), ((1, 5, True), []), ((1, 5, False), []), ((1, 6, True), []), ((1, 6, False), []), ((1, 7, True), []), ((1, 7, False), []), ((1, 8, True), []), ((1, 8, False), []), ((1, 9, True), []), ((1, 9, False), []), ((1, 10, True), []), ((1, 10, False), []), ((2, 1, True), []), ((2, 1, False), []), ((2, 2, True), []), ((2, 2, False), []), ((2, 3, True), []), ((2, 3, False), []), ((2, 4, True), []), ((2, 4, False), []), ((2, 5, True), []), ((2, 5, False), []), ((2, 6, True), []), ((2, 6, False), []), ((2, 7, True), []), ((2, 7, False), []), ((2, 8, True), []), ((2, 8, False), []), ((2, 9, True), []), ((2, 9, False), []), ((2, 10, True), []), ((2, 10, False), []), ((3, 1, True), []), ((3, 1, False), []), ((3, 2, True), []), ((3, 2, False), []), ((3, 3, True), []), ((3, 3, False), []), ((3, 4, True), []), ((3, 4, False), []), ((3, 5, True), []), ((3, 5, False), []), ((3, 6, True), []), ((3, 6, False), []), ((3, 7, True), []), ((3, 7, False), []), ((3, 8, True), []), ((3, 8, False), []), ((3, 9, True), []), ((3, 9, False), []), ((3, 10, True), []), ((3, 10, False), []), ((4, 1, True), []), ((4, 1, False), []), ((4, 2, True), []), ((4, 2, False), []), ((4, 3, True), []), ((4, 3, False), []), ((4, 4, True), []), ((4, 4, False), []), ((4, 5, True), []), ((4, 5, False), []), ((4, 6, True), []), ((4, 6, False), []), ((4, 7, True), []), ((4, 7, False), []), ((4, 8, True), []), ((4, 8, False), []), ((4, 9, True), []), ((4, 9, False), []), ((4, 10, True), []), ((4, 10, False), []), ((5, 1, True), []), ((5, 1, False), []), ((5, 2, True), []), ((5, 2, False), []), ((5, 3, True), []), ((5, 3, False), []), ((5, 4, True), []), ((5, 4, False), []), ((5, 5, True), []), ((5, 5, False), []), ((5, 6, True), []), ((5, 6, False), []), ((5, 7, True), []), ((5, 7, False), []), ((5, 8, True), []), ((5, 8, False), []), ((5, 9, True), []), ((5, 9, False), []), ((5, 10, True), []), ((5, 10, False), []), ((6, 1, True), []), ((6, 1, False), []), ((6, 2, True), []), ((6, 2, False), []), ((6, 3, True), []), ((6, 3, False), []), ((6, 4, True), []), ((6, 4, False), []), ((6, 5, True), []), ((6, 5, False), []), ((6, 6, True), []), ((6, 6, False), []), ((6, 7, True), []), ((6, 7, False), []), ((6, 8, True), []), ((6, 8, False), []), ((6, 9, True), []), ((6, 9, False), []), ((6, 10, True), []), ((6, 10, False), []), ((7, 1, True), []), ((7, 1, False), []), ((7, 2, True), []), ((7, 2, False), []), ((7, 3, True), []), ((7, 3, False), []), ((7, 4, True), []), ((7, 4, False), []), ((7, 5, True), []), ((7, 5, False), []), ((7, 6, True), []), ((7, 6, False), []), ((7, 7, True), []), ((7, 7, False), []), ((7, 8, True), []), ((7, 8, False), []), ((7, 9, True), []), ((7, 9, False), []), ((7, 10, True), []), ((7, 10, False), []), ((8, 1, True), []), ((8, 1, False), []), ((8, 2, True), []), ((8, 2, False), []), ((8, 3, True), []), ((8, 3, False), []), ((8, 4, True), []), ((8, 4, False), []), ((8, 5, True), []), ((8, 5, False), []), ((8, 6, True), []), ((8, 6, False), []), ((8, 7, True), []), ((8, 7, False), []), ((8, 8, True), []), ((8, 8, False), []), ((8, 9, True), []), ((8, 9, False), []), ((8, 10, True), []), ((8, 10, False), []), ((9, 1, True), []), ((9, 1, False), []), ((9, 2, True), []), ((9, 2, False), []), ((9, 3, True), []), ((9, 3, False), []), ((9, 4, True), []), ((9, 4, False), []), ((9, 5, True), []), ((9, 5, False), []), ((9, 6, True), []), ((9, 6, False), []), ((9, 7, True), []), ((9, 7, False), []), ((9, 8, True), []), ((9, 8, False), []), ((9, 9, True), []), ((9, 9, False), []), ((9, 10, True), []), ((9, 10, False), []), ((10, 1, True), []), ((10, 1, False), []), ((10, 2, True), []), ((10, 2, False), []), ((10, 3, True), []), ((10, 3, False), []), ((10, 4, True), []), ((10, 4, False), []), ((10, 5, True), []), ((10, 5, False), []), ((10, 6, True), []), ((10, 6, False), []), ((10, 7, True), []), ((10, 7, False), []), ((10, 8, True), []), ((10, 8, False), []), ((10, 9, True), []), ((10, 9, False), []), ((10, 10, True), []), ((10, 10, False), []), ((11, 1, True), []), ((11, 1, False), []), ((11, 2, True), []), ((11, 2, False), []), ((11, 3, True), []), ((11, 3, False), []), ((11, 4, True), []), ((11, 4, False), []), ((11, 5, True), []), ((11, 5, False), []), ((11, 6, True), []), ((11, 6, False), []), ((11, 7, True), []), ((11, 7, False), []), ((11, 8, True), []), ((11, 8, False), []), ((11, 9, True), []), ((11, 9, False), []), ((11, 10, True), []), ((11, 10, False), []), ((12, 1, True), []), ((12, 1, False), []), ((12, 2, True), []), ((12, 2, False), []), ((12, 3, True), []), ((12, 3, False), []), ((12, 4, True), []), ((12, 4, False), []), ((12, 5, True), []), ((12, 5, False), []), ((12, 6, True), []), ((12, 6, False), []), ((12, 7, True), []), ((12, 7, False), []), ((12, 8, True), []), ((12, 8, False), []), ((12, 9, True), []), ((12, 9, False), []), ((12, 10, True), []), ((12, 10, False), []), ((13, 1, True), []), ((13, 1, False), []), ((13, 2, True), []), ((13, 2, False), []), ((13, 3, True), []), ((13, 3, False), []), ((13, 4, True), []), ((13, 4, False), []), ((13, 5, True), []), ((13, 5, False), []), ((13, 6, True), []), ((13, 6, False), []), ((13, 7, True), []), ((13, 7, False), []), ((13, 8, True), []), ((13, 8, False), []), ((13, 9, True), []), ((13, 9, False), []), ((13, 10, True), []), ((13, 10, False), []), ((14, 1, True), []), ((14, 1, False), []), ((14, 2, True), []), ((14, 2, False), []), ((14, 3, True), []), ((14, 3, False), []), ((14, 4, True), []), ((14, 4, False), []), ((14, 5, True), []), ((14, 5, False), []), ((14, 6, True), []), ((14, 6, False), []), ((14, 7, True), []), ((14, 7, False), []), ((14, 8, True), []), ((14, 8, False), []), ((14, 9, True), []), ((14, 9, False), []), ((14, 10, True), []), ((14, 10, False), []), ((15, 1, True), []), ((15, 1, False), []), ((15, 2, True), []), ((15, 2, False), []), ((15, 3, True), []), ((15, 3, False), []), ((15, 4, True), []), ((15, 4, False), []), ((15, 5, True), []), ((15, 5, False), []), ((15, 6, True), []), ((15, 6, False), []), ((15, 7, True), []), ((15, 7, False), []), ((15, 8, True), []), ((15, 8, False), []), ((15, 9, True), []), ((15, 9, False), []), ((15, 10, True), []), ((15, 10, False), []), ((16, 1, True), []), ((16, 1, False), []), ((16, 2, True), []), ((16, 2, False), []), ((16, 3, True), []), ((16, 3, False), []), ((16, 4, True), []), ((16, 4, False), []), ((16, 5, True), []), ((16, 5, False), []), ((16, 6, True), []), ((16, 6, False), []), ((16, 7, True), []), ((16, 7, False), []), ((16, 8, True), []), ((16, 8, False), []), ((16, 9, True), []), ((16, 9, False), []), ((16, 10, True), []), ((16, 10, False), []), ((17, 1, True), []), ((17, 1, False), []), ((17, 2, True), []), ((17, 2, False), []), ((17, 3, True), []), ((17, 3, False), []), ((17, 4, True), []), ((17, 4, False), []), ((17, 5, True), []), ((17, 5, False), []), ((17, 6, True), []), ((17, 6, False), []), ((17, 7, True), []), ((17, 7, False), []), ((17, 8, True), []), ((17, 8, False), []), ((17, 9, True), []), ((17, 9, False), []), ((17, 10, True), []), ((17, 10, False), []), ((18, 1, True), []), ((18, 1, False), []), ((18, 2, True), []), ((18, 2, False), []), ((18, 3, True), []), ((18, 3, False), []), ((18, 4, True), []), ((18, 4, False), []), ((18, 5, True), []), ((18, 5, False), []), ((18, 6, True), []), ((18, 6, False), []), ((18, 7, True), []), ((18, 7, False), []), ((18, 8, True), []), ((18, 8, False), []), ((18, 9, True), []), ((18, 9, False), []), ((18, 10, True), []), ((18, 10, False), []), ((19, 1, True), []), ((19, 1, False), []), ((19, 2, True), []), ((19, 2, False), []), ((19, 3, True), []), ((19, 3, False), []), ((19, 4, True), []), ((19, 4, False), []), ((19, 5, True), []), ((19, 5, False), []), ((19, 6, True), []), ((19, 6, False), []), ((19, 7, True), []), ((19, 7, False), []), ((19, 8, True), []), ((19, 8, False), []), ((19, 9, True), []), ((19, 9, False), []), ((19, 10, True), []), ((19, 10, False), []), ((20, 1, True), []), ((20, 1, False), []), ((20, 2, True), []), ((20, 2, False), []), ((20, 3, True), []), ((20, 3, False), []), ((20, 4, True), []), ((20, 4, False), []), ((20, 5, True), []), ((20, 5, False), []), ((20, 6, True), []), ((20, 6, False), []), ((20, 7, True), []), ((20, 7, False), []), ((20, 8, True), []), ((20, 8, False), []), ((20, 9, True), []), ((20, 9, False), []), ((20, 10, True), []), ((20, 10, False), []), ((21, 1, True), []), ((21, 1, False), []), ((21, 2, True), []), ((21, 2, False), []), ((21, 3, True), []), ((21, 3, False), []), ((21, 4, True), []), ((21, 4, False), []), ((21, 5, True), []), ((21, 5, False), []), ((21, 6, True), []), ((21, 6, False), []), ((21, 7, True), []), ((21, 7, False), []), ((21, 8, True), []), ((21, 8, False), []), ((21, 9, True), []), ((21, 9, False), []), ((21, 10, True), []), ((21, 10, False), []), ((22, 1, True), []), ((22, 1, False), []), ((22, 2, True), []), ((22, 2, False), []), ((22, 3, True), []), ((22, 3, False), []), ((22, 4, True), []), ((22, 4, False), []), ((22, 5, True), []), ((22, 5, False), []), ((22, 6, True), []), ((22, 6, False), []), ((22, 7, True), []), ((22, 7, False), []), ((22, 8, True), []), ((22, 8, False), []), ((22, 9, True), []), ((22, 9, False), []), ((22, 10, True), []), ((22, 10, False), []), ((23, 1, True), []), ((23, 1, False), []), ((23, 2, True), []), ((23, 2, False), []), ((23, 3, True), []), ((23, 3, False), []), ((23, 4, True), []), ((23, 4, False), []), ((23, 5, True), []), ((23, 5, False), []), ((23, 6, True), []), ((23, 6, False), []), ((23, 7, True), []), ((23, 7, False), []), ((23, 8, True), []), ((23, 8, False), []), ((23, 9, True), []), ((23, 9, False), []), ((23, 10, True), []), ((23, 10, False), []), ((24, 1, True), []), ((24, 1, False), []), ((24, 2, True), []), ((24, 2, False), []), ((24, 3, True), []), ((24, 3, False), []), ((24, 4, True), []), ((24, 4, False), []), ((24, 5, True), []), ((24, 5, False), []), ((24, 6, True), []), ((24, 6, False), []), ((24, 7, True), []), ((24, 7, False), []), ((24, 8, True), []), ((24, 8, False), []), ((24, 9, True), []), ((24, 9, False), []), ((24, 10, True), []), ((24, 10, False), []), ((25, 1, True), []), ((25, 1, False), []), ((25, 2, True), []), ((25, 2, False), []), ((25, 3, True), []), ((25, 3, False), []), ((25, 4, True), []), ((25, 4, False), []), ((25, 5, True), []), ((25, 5, False), []), ((25, 6, True), []), ((25, 6, False), []), ((25, 7, True), []), ((25, 7, False), []), ((25, 8, True), []), ((25, 8, False), []), ((25, 9, True), []), ((25, 9, False), []), ((25, 10, True), []), ((25, 10, False), []), ((26, 1, True), []), ((26, 1, False), []), ((26, 2, True), []), ((26, 2, False), []), ((26, 3, True), []), ((26, 3, False), []), ((26, 4, True), []), ((26, 4, False), []), ((26, 5, True), []), ((26, 5, False), []), ((26, 6, True), []), ((26, 6, False), []), ((26, 7, True), []), ((26, 7, False), []), ((26, 8, True), []), ((26, 8, False), []), ((26, 9, True), []), ((26, 9, False), []), ((26, 10, True), []), ((26, 10, False), []), ((27, 1, True), []), ((27, 1, False), []), ((27, 2, True), []), ((27, 2, False), []), ((27, 3, True), []), ((27, 3, False), []), ((27, 4, True), []), ((27, 4, False), []), ((27, 5, True), []), ((27, 5, False), []), ((27, 6, True), []), ((27, 6, False), []), ((27, 7, True), []), ((27, 7, False), []), ((27, 8, True), []), ((27, 8, False), []), ((27, 9, True), []), ((27, 9, False), []), ((27, 10, True), []), ((27, 10, False), []), ((28, 1, True), []), ((28, 1, False), []), ((28, 2, True), []), ((28, 2, False), []), ((28, 3, True), []), ((28, 3, False), []), ((28, 4, True), []), ((28, 4, False), []), ((28, 5, True), []), ((28, 5, False), []), ((28, 6, True), []), ((28, 6, False), []), ((28, 7, True), []), ((28, 7, False), []), ((28, 8, True), []), ((28, 8, False), []), ((28, 9, True), []), ((28, 9, False), []), ((28, 10, True), []), ((28, 10, False), []), ((29, 1, True), []), ((29, 1, False), []), ((29, 2, True), []), ((29, 2, False), []), ((29, 3, True), []), ((29, 3, False), []), ((29, 4, True), []), ((29, 4, False), []), ((29, 5, True), []), ((29, 5, False), []), ((29, 6, True), []), ((29, 6, False), []), ((29, 7, True), []), ((29, 7, False), []), ((29, 8, True), []), ((29, 8, False), []), ((29, 9, True), []), ((29, 9, False), []), ((29, 10, True), []), ((29, 10, False), []), ((30, 1, True), []), ((30, 1, False), []), ((30, 2, True), []), ((30, 2, False), []), ((30, 3, True), []), ((30, 3, False), []), ((30, 4, True), []), ((30, 4, False), []), ((30, 5, True), []), ((30, 5, False), []), ((30, 6, True), []), ((30, 6, False), []), ((30, 7, True), []), ((30, 7, False), []), ((30, 8, True), []), ((30, 8, False), []), ((30, 9, True), []), ((30, 9, False), []), ((30, 10, True), []), ((30, 10, False), []), ((31, 1, True), []), ((31, 1, False), []), ((31, 2, True), []), ((31, 2, False), []), ((31, 3, True), []), ((31, 3, False), []), ((31, 4, True), []), ((31, 4, False), []), ((31, 5, True), []), ((31, 5, False), []), ((31, 6, True), []), ((31, 6, False), []), ((31, 7, True), []), ((31, 7, False), []), ((31, 8, True), []), ((31, 8, False), []), ((31, 9, True), []), ((31, 9, False), []), ((31, 10, True), []), ((31, 10, False), [])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and initialize a new temporary returns function R(S_t, A_t) with empty lists\n",
    "rsTemp = returnsS.returnsStateBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce)\n",
    "rsTemp.R_s.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rsTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Target Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent's target policy is denoted as $\\pi$ and is used to represent the conditional probability mass function $f_{A|S}$ of actions over the states:<br>\n",
    "$$\\large \\pi\\coloneqq f_{A|S}$$\n",
    "\n",
    "We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large \\pi(a|s)=f_{A|S}(a|s)=P(A=a|S=s)$$\n",
    "\n",
    "Recall from the defination of the Action Space $A$ from above, that $A$ follows a Bernoulli distribution where $P(A=a)$  We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large \\pi(a_{t}|s_{t})\n",
    "=\n",
    "\\begin{cases}\n",
    "0\\quad\\quad\\quad \\text{if } Q(s_{t}, a_{t}=0)\\lt Q(s_{t}, a_{t}=1) \\\\\n",
    "1\\quad\\quad\\quad \\text{if }Q(s_{t}, a_{t}=0)\\gt Q(s_{t}, a_{t}=1)\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary pi is a member of the [policyBlackjack class](#policyBlackjack.ipynb) and uses the key={((playerSum, dealerShows, usableAce), action)} : value={$\\pi(a_{t}\\,\\,|\\,\\,s_{t})$} pairs and will be used later within the Monte Carlo Prediction State Value Off Policy Ordinary Importance Sampling First Visit algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a temporary policy pi(s)\n",
    "piTemp = tPolicy.targetPolicyBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "piTemp.pi.items()\n",
    "del piTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Behavior Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent's behavior policy is denoted as $b$ and is used to represent the conditional probability mass function $f_{A|S}$ of actions over the states:<br>\n",
    "$$\\large b\\coloneqq f_{A|S}$$\n",
    "\n",
    "Our agent must, at timestep $t$, choose an action $a\\in A$ (either '0' to 'stick' or '1' to 'hit') to move the from its current state $s\\in S$ to its next state $s'\\in S$.\n",
    "\n",
    "We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large b(a|s)=f_{A|S}(a|s)=P(A=a|S=s)$$\n",
    "\n",
    "Recall from the defination of the Action Space $A$ from above, that $A$ follows a Bernoulli distribution where $P(A=a)$  We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large b(a_{t}|s_{t})\n",
    "=\n",
    "\\begin{cases}\n",
    "0\\quad\\quad\\quad\\text{if }f(s_{t}, a_{t}=0)\\lt f(s_{t}, a_{t}=1) \\\\\n",
    "1\\quad\\quad\\quad\\text{if }f(s_{t}, a_{t}=0)\\ge f(s_{t}, a_{t}=1)\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>State Value Function</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bellman eqautions, the state value function at time $t$, $v_{\\pi}(s_{t})$, within an episode represents the expected return when in state $s_{t}\\in S$ according to policy $\\pi$. [\\[4\\]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad v_\\pi(s_t)\\coloneqq\\mathbb{E} [G_t | S_t=s_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma G_{t+1} | S_t=s_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\,\\,|\\,\\,S_{t}=s_{t}]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\sum\\limits_{a\\in A}\\pi(a_{t}|s{t})\\,\\sum\\limits_{s_{t+1}} \\sum\\limits_{r} P(s_{t+1},\\,r\\,|\\,s_{t},\\,a_{t})[r+\\gamma v_\\pi(s_{t+1})]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Prediction methods such as Off Policy Ordinary Importance Sampling First Visit allow us to make an estimate $V(s_{t})$ of the state value function $v_\\pi(s_{t})$ which will be demonstrated later and below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python dictionary V_s is a member of the [stateValue class](#stateValueBlackjack.ipynb) and uses the key={(playerSum, dealerShows, usableAce)} : value={$V(s_{t})$} pairs which can be used to store the state value function values given policy $\\pi$ at any time step t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and initialize a new temporary action value function Q(S_t, A_t)\n",
    "vsTemp = stateValue.stateValueBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce)\n",
    "vsTemp.V_s.items()\n",
    "del vsTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Action Value Function</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bellman eqautions, the action value function $Q(s_{t}, a_{t})$ at time $t$ within an episode represents the expected return when choosing action $a_{t}\\in A$ when in state $s_{t}\\in S$ according to policy $\\pi_*(a_{t}|s_{t})$. [\\[4\\]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad q_\\pi(s_t, a_t)\\coloneqq\\mathbb{E} [G_t | S_t=s_t, A_t=a_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma G_{t+1} | S_t=s_t, A_t=a_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\,\\,|\\,\\,S_{t}=s_{t},\\,\\,A_{t}=a_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\sum\\limits_{s_{t+1}} \\sum\\limits_{r} P(s_{t+1}, r| s_{t}, a_{t})[r+\\gamma v_\\pi(s_{t+1})]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Prediction methods such as Off Policy Ordinary Importance Sampling First Visit allow us to make an estimate $Q(s_{t}, a_{t})$ of the state value function $q_\\pi(s_{t}, a_{t})$ which will be demonstrated later and below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python dictionary Q_s_a is a member of the [stateActionValue class](#stateActionValueBlackjack.ipynb) and uses the key={((playerSum, dealerShows, usableAce), action)} : value={$Q(s_{t}, a_{t})$} pairs and will be used later within the Monte Carlo Prediction Action Value Off Policy Ordinary Importance Sampling First Visit algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and initialize a new temporary action value function Q(S_t, A_t)\n",
    "qsaTemp = actionValue.stateActionValueBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "qsaTemp.Q_s_a.items()\n",
    "del qsaTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MonteCarloPrediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_FirstVisit_Blackjack\"></a>\n",
    "<h2>Monte Carlo Prediction State Value Off Policy Ordinary Importance Sampling First Visit Algorithm</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent must, at timestep $t$, choose an action $a\\in A$ (either '0' to 'stick' or '1' to 'hit') to move the from its current state $s\\in S$ to its next state $s'\\in S$ within a Blackjack episode. As in many reinforcement learning MDP problems, the the choice of action taken by an agent at each time step must either exploit the estimated optimal action $a_{t}\\in A$ given the current state $s_{t}\\in S$ or explore the action space for actions other than the optimal action in order to see if there are other actions with higher expected returns that may be better to choose going forward.\n",
    "As discussed in Barto's chapter on Monte Carlo methods for State Value function estimation using off policy mehtods [\\[5\\]](#References), the Monte Carlo Prediction State Value Off Policy Ordinary Importance Sampling First Visit algorithim uses target policy $\\pi$ to exploit the optimal action, and a behavior policy $b$ to explore the action space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off policy algorithms learn from data that is not a result of the target policy $\\pi$ but rather some behavior policy $b$. As discussed in [\\[5\\]](#References). This notebook will demonstrate the problem of estimating the state value function $V(S)\\approx v_{\\pi}(S)$ by using an Ordinary Importance Sampling First Visit algorithm that uses fixed-given target and behavior policies $\\pi$ and $b$ respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\rho$ transforms the returns while following the behavior policy $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large v_{\\pi}(s_{t})\\,\\,=\\,\\,\\mathbb{E_{b}} [\\rho_{t:T-1}G_{t}|S_{t}=s_{t}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of taking a trajectory according to the target policy $\\pi$ is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large P(A_{t},\\,\\,S_{t+1},\\,\\,A_{t+1},\\,\\,\\dots,\\,\\,S_{T}\\,\\,|\\,\\,S_{t},\\,\\,A_{t:T-1}\\sim\\pi)$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad =\\pi(A_{t}|S_{t})\\,\\,P(S_{t+1}|S_{t},\\,\\,A_{t})\\,\\,\\pi(A_{t+1}|S_{t+}),\\,\\,\\cdots,\\,\\,P(S_{T}|S_{T-1},\\,\\,A_{T-1})$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\quad\\prod\\limits_{k=t}^{T-1} \\pi(A_{k}|S_{k})\\,\\,P(S_{k+1}|S_{k},\\,\\,A_{k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the probability of taking a trajectory according to the behavior policy $b$ is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large P(A_{t},\\,\\,S_{t+1},\\,\\,A_{t+1},\\,\\,\\dots,\\,\\,S_{T}\\,\\,|\\,\\,S_{t},\\,\\,A_{t:T-1}\\sim b)$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=b(A_{t}|S_{t})\\,\\,P(S_{t+1}|S_{t},\\,\\,A_{t})\\,\\,b(A_{t+1}|S_{t+}),\\,\\,\\cdots,\\,\\,P(S_{T}|S_{T-1},\\,\\,A_{T-1})$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\quad\\prod\\limits_{k=t}^{T-1} b(A_{k}|S_{k})\\,\\,P(S_{k+1}|S_{k},\\,\\,A_{k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the ratio of these two probailites we obtain the Ordingary Importance Sampling Ratio $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large\\rho_{t:T-1}=\\frac{\\prod\\limits_{k=t}^{T-1} \\pi(A_{k}|S_{k})P(S_{k+1}|S_{k},\\,\\,A_{k})}{\\prod\\limits_{k=t}^{T-1} b(A_{k}|S_{k})P(S_{k+1}|S_{k},\\,\\,A_{k})}\\,\\,=\\,\\,\\prod\\limits_{k=t}^{T-1}\\frac{\\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ordinary Importance Sampling Ratio $\\rho$ is used to scale the returns which we then take the average of those scaled returns obtained by following behavior policy $b$. These averaged scaled returns for following policy $b$ provide an estimate $V(S_{t})\\approx v_{\\pi}(S_{t})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large V(S_{t})=\\frac{\\sum\\limits_{t\\in \\tau(s)}\\rho_{t:T(t)-1}G_{t}}{|\\tau(s)|}$$\n",
    "$$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\text{where }\\tau(s)\\text{ is the set of all time steps in which state }s\\text{ was visited.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the State Value function when following policy $\\pi$ while only seeing the outcomes of following policy $b$, the Monte Carlo Prediction State Value Off Policy Ordinary Importance Sampling First Visit algorithm assumes that every action taken under $\\pi$ during an MDP is also taken occasionally under the behavior policy $b$ with some non-zero porobability.[\\[5\\]](#References) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(a_{t}|s_{t})\\gt 0 \\rightarrow b(a_{t}|s_{t})\\gt0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode for the Off Policy Ordinary Importnace Sampling State Value First Visit algorithm which estimates the state value function $V(s_{t})$ is found below and as described in the Barto et. al. [\\[6\\]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\text{Monte Carlo Prediction State Value Off Policy Ordinary Importance Sampling First Visit Algorithm}$<br>\n",
    "\n",
    "$\\large\\quad\\quad\\text{Inputs:}\\,\\,\\text{target policy\\,\\,}\\pi,\\,\\,\\text{behavior policy }b,\\,\\,environment,\\,\\,firstVisit=True,\\,\\,numEpisodes,\\,\\,discountFactor$<br>\n",
    "\n",
    "$\\large\\quad\\quad\\text{ 1.}\\quad V(s)\\leftarrow\\text{Randomly chosen state value for all states }s\\in S$<br>\n",
    "$\\large\\quad\\quad\\text{ 2.}\\quad R(s)\\leftarrow\\text{Empty list of returns for each state }(S=s)$<br>\n",
    "$\\large\\quad\\quad\\text{ 3.}\\quad\\text{Loop forever (for each episode)}$:<br>\n",
    "$\\large\\quad\\quad\\text{ 4.}\\quad\\quad\\text{Allow the environment to provide }S_{0}\\in S\\text{ and choose action }A_{0}\\in A(S_{0})\\text{ randomly from the conditional distribution }b$<br>\n",
    "$\\large\\quad\\quad\\text{ 5.}\\quad\\quad\\text{Generate an episode from }S_{0}, A_{0},\\text{ following }b\\colon S_{0}, A_{0}, R_{1},..., S_{T-1}, A_{T-1}, R_{T}$<br>\n",
    "$\\large\\quad\\quad\\text{ 6.}\\quad\\quad\\,G\\leftarrow\\,0\\quad W\\leftarrow 1$<br>\n",
    "$\\large\\quad\\quad\\text{ 7.}\\quad\\quad\\text{Loop for each step of episode, }t = T-1,\\,T-2,\\,\\dots,\\,0$<br>\n",
    "$\\large\\quad\\quad\\text{ 8.}\\quad\\quad\\quad\\,\\,\\,G\\leftarrow\\,\\gamma WG+R_{t+1}$<br>\n",
    "$\\large\\quad\\quad\\text{ 9.}\\quad\\quad\\quad\\text{ if(firstVisit==False or (firstVisit==True and pair (}s_{t}, a_{t}\\text{) does not appear in } S_{0}, A_{0}, S_{1}, A_{1},\\,\\dots,\\,S_{t-1},\\,A_{t-1}\\text{))}\\colon$<br>\n",
    "$\\large\\quad\\quad\\text{10.}\\quad\\quad\\quad\\quad\\text{Append }G\\text{ to } R(S_{t})$<br>\n",
    "$\\large\\quad\\quad\\text{11.}\\quad\\quad\\quad\\quad V(S_{t})\\leftarrow\\text{average}(R(S_{t}))$<br>\n",
    "$\\large\\quad\\quad\\text{12.}\\quad\\quad\\quad\\quad W\\leftarrow\\,\\,W\\,\\,\\pi(A_{t}|S_{t}) / b(A_{t}|S_{t})$<br>\n",
    "$\\large\\quad\\quad\\text{13.}\\quad\\text{Return } V(S)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Don't forget that $R_{t+1}=R_{T}\\,\\,\\forall\\,0\\le\\,t\\lt\\,T$ when using the Blackjack env as we do in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloPrediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_Blackjack(targetPolicy, behaviorPolicy, env, V_s, R_s, firstVisit=True, numEpisodes=1, discountFactor=1):\n",
    "    # 3. Run numEpisodes episodes (instead of looping forever as described in the pseudocode above)\n",
    "    for _ in range(numEpisodes):\n",
    "        # 4. Create a list to track the observations (i.e. states) and actions taken during an episode\n",
    "        episodeStateAction = []\n",
    "        # Set natural flag to ensure an action is needed to be taken by the agent\n",
    "        natural = False\n",
    "        # 4. Reset the envirionment and generate a random intitial state S_0\n",
    "        observation, info = env.reset(seed=(int)(datetime.now().timestamp() * 1000000))\n",
    "        # 4. Choose an action from policy pi based upon the observation \n",
    "        action = 0 if ( behaviorPolicy.b[((observation), 0)] > behaviorPolicy.b[((observation), 1)] ) else 1            \n",
    "        # Set the done flag to false\n",
    "        done = False\n",
    "        # 5. Generate an episode from S_0 and A_0 follwoing policy pi\n",
    "        while ( not done ):\n",
    "            # Push the observation (i.e. state) action pair to the episode as the key and add an empty list as the value\n",
    "            episodeStateAction.append((observation, action))\n",
    "            # Step to the next state by performing an action \n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            # Choose an action from policy pi based upon the observation \n",
    "            action = 0 if ( behaviorPolicy.b[((observation), 0)] > behaviorPolicy.b[((observation), 1)] ) else 1            \n",
    "            if ( terminated or truncated ):\n",
    "                done = True\n",
    "        # 6. Set the returns for the current episode to zero\n",
    "        G = 0\n",
    "        # 6. Set the odinary weight value W to one\n",
    "        W = 1.0\n",
    "        # Set T to the length of the episode list\n",
    "        # This number should be appropriate in order to maintain \n",
    "        # the episode sequence {S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T-1, A_T-1, R_T}\n",
    "        T = len(episodeStateAction)\n",
    "        reward_T = reward\n",
    "        # 7. Loop for each step of episode, t=T-1, T-2, ..., 0\n",
    "        for t in range(T-1, -1, -1):\n",
    "            # 8. Compute G_t for timestep t starting at t=T-1 until t=0\n",
    "            G = discountFactor * W * G + reward_T         \n",
    "            # 9. If (firstVisit is False) or (firstVisit is True and state-action pair is not in in S_0, A_0, S_1, A_1, ... S_t-1, A_t-1)  \n",
    "            if ( (firstVisit==False) or ((firstVisit == True) and (not (episodeStateAction[t] in episodeStateAction[:t]))) ):\n",
    "                # 10. Append G to returns R(S_t)\n",
    "                R_s.R_s[(episodeStateAction[t][0])].append(G)\n",
    "                # 11. Set V(S_t) to the average of Returns(S_t)\n",
    "                V_s.V_s[(episodeStateAction[t][0])] = sum(R_s.R_s[episodeStateAction[t][0]]) / len(R_s.R_s[episodeStateAction[t][0]])\n",
    "                # 12. Update W\n",
    "                W = W * targetPolicy.pi[(episodeStateAction[t])] / behaviorPolicy.b[(episodeStateAction[t])]         \n",
    "    return V_s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the discount factor (aka \"gamma\") used when computing returns\n",
    "discountFactor = 1.0\n",
    "\n",
    "# Set the number of runs\n",
    "numRuns = 1\n",
    "\n",
    "#episodes = [1, 10, 50, 100, 200, 500, 1000, 3000, 5000, 10000]\n",
    "episodes = [5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "for i in range(len(episodes)):\n",
    "    # Run an experiment j times\n",
    "    for runNum in range(numRuns):\n",
    "        # Set the environment \n",
    "        env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "        # Use the RecorEpisodeStatistics wrapper to track rewards and episode lengths\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=episodes[i])\n",
    "        # Open a pre-trained target policy that was serialized to json in [\\[for use as an example\n",
    "        with open('../../MonteCarloControl/MonteCarloControl_OnPolicy_ExploringStarts_FirstVisit_Blackjack/results/target_policy.pickle', 'rb') as handle:\n",
    "            pi = pickle.load(handle)\n",
    "        # Initialize policy b(s) to equal probabilities 0.5 for all s in S\n",
    "        b = bPolicy.behaviorPolicyBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "        # 1. Instantiate and initialize a new state value function V(S_t)\n",
    "        V_s = stateValue.stateValueBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce)\n",
    "        # 2. Instantiate and initialize a new returns function R(S_t) with empty lists\n",
    "        R_s = returnsS.returnsStateBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce)\n",
    "        # Compute the estimate of the state value function V\n",
    "        stateValueResult = MonteCarloPrediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_Blackjack(targetPolicy=pi, behaviorPolicy=b, env=env, V_s=V_s, R_s=R_s, firstVisit=True, numEpisodes=episodes[i], discountFactor=1.0)\n",
    "        # Set up the grids for plots of action value and policy when usable Ace is available\n",
    "        value_grid = h.create_state_value_grid(V_s=stateValueResult, usable_ace=True)\n",
    "        # Format a string for the title of the plot when there is a usable Ace available\n",
    "        title = \"MC Prediction State Value\\nOffPolicy Ordinary Importance Sampling First Visit\\n#Episodes=\" + str(episodes[i]) + \", \\u03B3=\" + str(discountFactor) + \"\\nRun# \" + str(runNum+1) + \", usableAce=T\\n\"\n",
    "        fileName =\"results/MC_Prediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_FirstVisit_Blackjack_Episodes_\" + str(episodes[i]) + \"_DiscountFactor_\" + str(discountFactor) + \"_Run_\" + str(runNum+1) + \"_usableAce_T.png\"\n",
    "        h.create_state_value_plot(value_grid=value_grid, title=title, fileName=fileName, numEpisodes=episodes[i], runNum=runNum, discountFactor=discountFactor, firstVisit=True, usableAce=True)\n",
    "        # Set up the grids for plots of action value and policy when usable Ace is not available\n",
    "        value_grid = h.create_state_value_grid(V_s=stateValueResult, usable_ace=False)\n",
    "        # Format a string for the title of the plot when there is a usable Ace is not available\n",
    "        title = \"MC Prediction State Value\\nOff Policy Ordinary Importance Sampling First Visit\\n#Episodes=\" + str(episodes[i]) + \", \\u03B3=\" + str(discountFactor) + \"\\nRun# \" + str(runNum+1) + \", usableAce=F\\n\"\n",
    "        fileName =\"results/MC_Prediction_StateValue_OffPolicy_Ordinary_ImportanceSampling_FirstVisit_Blackjack_Episodes_\" + str(episodes[i]) + \"_DiscountFactor_\" + str(discountFactor) + \"_Run_\" + str(runNum+1) + \"_usableAce_F.png\"\n",
    "        h.create_state_value_plot(value_grid=value_grid, title=title, fileName=fileName, numEpisodes=episodes[i], runNum=runNum, discountFactor=discountFactor, firstVisit=True, usableAce=False)\n",
    "        # Close the environment to clean up resources used by the environment\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "<h2>References</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 58. \n",
    "\n",
    "2. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 99.\n",
    "\n",
    "3. https://gymnasium.farama.org/environments/toy_text/blackjack/\n",
    "\n",
    "4. https://github.com/openai/gym/issues/1410\n",
    "\n",
    "5. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 103-106. \n",
    "\n",
    "6. White A., White M. \"Sample-based Learnning Methods: Week 1\", Sample Based Learning Methods, \"Reinforcement Learning\" Specialization, www.coursera.org/learn/sample-based-learngin-methods/home/week1.\n",
    "\n",
    "7. [Monte Carlo Control On Policy Exploring Starts First Visit algorithm](#../MonteCarloControl/MonteCarloControl_OnPolicy_ExploringStarts_FirstVisit_Blackjack/MonteCarloControl_OnPolicy_ExploringStarts_FirstVisit_Blackjack.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_python_3_9_kernel",
   "language": "python",
   "name": "basic_python_3_9_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56a450c7dfd4e5ec1e847f4263cc8e8bcabd06d76b9dab54510bb1f50cf86976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
