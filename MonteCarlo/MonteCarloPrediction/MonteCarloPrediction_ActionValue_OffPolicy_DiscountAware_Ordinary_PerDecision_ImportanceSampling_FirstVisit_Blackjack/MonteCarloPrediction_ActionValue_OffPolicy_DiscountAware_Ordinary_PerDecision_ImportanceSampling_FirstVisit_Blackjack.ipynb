{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MonteCarloPrediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_FirstVisit_Blackjack\"></a>\n",
    "<h1>Monte Carlo Prediction of Action Value Function: Using Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit for Blackjack</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo prediction algorithms, such as Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit, can be used to estimate the action-value functions, $Q(S, A)$, described by the Bellman equations.[\\[1\\]](#references)  This notebook contains a funtions that compute estimates for $Q(S_{t}, A_{t})\\approx q_{\\pi}(s_{t}, a_{t})$ using the [Off Policy Discount Aware Orinary Per Decision Importance Sampling First Visit](#MonteCarloPrediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_FirstVisit_Blackjack) algorithm for the Blackjack problem described in Example 5.3 of the text by Sutton and Barto. [\\[2\\]](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will require the following python modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image\n",
    "import import_ipynb\n",
    "import ipynb.fs.full.policyBlackjack as tPolicy\n",
    "import ipynb.fs.full.targetPolicyBlackjack as tPolicy\n",
    "import ipynb.fs.full.behaviorPolicyBlackjack as bPolicy\n",
    "import ipynb.fs.full.returnsStateActionBlackjack as returnsSA\n",
    "import ipynb.fs.full.stateActionValueBlackjack as actionValue\n",
    "import ipynb.fs.full.helpers_MonteCarloPrediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_FirstVisit_Blackjack as h\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Environment: Blackjack</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: State Space</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state of a Blackjack game, represented by the random variable $S$,  consists of a 3-tuple containing 1.) the player's current score via summing the values of the cards in the player's hand, 2.) the value of the card shown by the dealer and 3.) whether the player is holding a usable ace. [\\[2\\]](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large S\\coloneqq\\{(playerSum,\\,\\,dealerShowing,\\,\\,usableAce)\\in\\mathbb{Z}^{3}: (0\\le playerSum \\le 32)\\cap(0\\le dealerShowing \\le 11)\\cap (0\\le usableAce\\le 2)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The state space (often refered to as the \"observation space\" within the Gymnasium documentation) covers instances of states that are not reachable in any Blackjack game. More specifically, player sums of 0 or 1 and dealer showing values of 0 are not possible given the constraints of the environment (i.e. the game of Blackjack). This was a design choice made by the Gym API developers (while still being maintained by OpenAI) in order to faciliate \"easier indexing for table based algorithms\".  processing of numerical results within the environment API. [\\[3\\]](#references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state spaces for player sum, dealer showing card, and usable ace\n",
    "# Note: Gymnasium's observation space is a Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
    "# which contains observations that are not possible. There are very few of them so continue\n",
    "# to set Q_s_a values according to the observation space assumed by Gymnasium\n",
    "# Note: the observation space contains unreachable states. This was a choice made by\n",
    "# the OpenAI developers to facilitate easy indexing. See https://github.com/opoenai/gym/issues/1410\n",
    "# for more information.\n",
    "stateSpacePlayerSum = [i for i in range(32)]\n",
    "stateSpaceDealerShows = [i for i in range(1, 11)]\n",
    "stateSpaceUsableAce = [True, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these enumerations of state values later on. For now let's talk about the Action Space of the Blackjack environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: Action Space</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent may choose to either 'hit' or 'stick', at each timepoint $t$ during an episode.   Each action taken by the agent (i.e. to either 'stick' or 'hit') is represented by an instance of the random variable $A$ which is sampled from from the Bernoulli distribution.. and there are two actions ('Hit' or 'Stick') available to the agent. [\\[2\\]](#references)<br>\n",
    "$$\\large A\\coloneqq\\{a\\in\\{0, 1\\} : a=0\\text{ when player 'sticks'} \\cap a=1\\text{ when player 'hits'}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create some lists to help us build the state and action spaces later on within the exploring starts algorithm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the action space (0='Stick', 1='Hit') according to the Gymnasium documentation\n",
    "actionSpace = [i for i in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this enumeration of actions within the Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm later below. For now let's talk about how the Blackjack environment manages episdoes and time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: Episodes and Time Steps</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Blackjack environment within the Gymnasium API, an episode consists of what is commonly called a game in a real-life Blackjack scenario. Each episode consists of $T$ timesteps {$t_{0}$, $t_{1}$, $\\dots$,$t_{T-1}$}. The initial state $s_{t}$ is provided by the Blackjack environment each time the environment is reset. The Monte Carlo Prediction State Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit Blackjack algorithm randomly assigns an action $a_{t}$ which then form the state-action pair ($S_{t}=s_{t}$, $A_{t}=a_{t}$). Each episode has the following form:<br>\n",
    "$$\\large\\text{Episode } \\coloneqq \\{ S_{t},\\,\\,A_{t},\\,\\,R_{t+1},\\,\\,S_{t+1},\\,\\,A_{t+1},\\,\\,R_{t+2},\\,\\,\\dots,\\,\\,S_{T-1},\\,\\,A_{T-1},\\,\\,R_{T}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this notebook exercise, the index of the time step immediately following the end of an episode (i.e. the first time step of the next episode) is set to $t=T+1$. $t$ is not set back to zero before starting the next episode. See [\\[9\\]](#References) for the details of how time indexing can be handled when doing Importance Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Environment Blackjack: Rewards and Returns</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward calculations are carried out at the end of each episode when using the Monte Carlo Prediction Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm.  Each game (i.e. episode) concludes with assignment of a reward to the random variable $R_{T}$ according to the following five outcomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\large R_T\n",
    "= \n",
    "\\begin{cases}\n",
    "-1\\quad\\text{dealer wins} \\\\\n",
    "0\\quad\\quad\\text{draw} \\\\\n",
    "1\\quad\\quad\\text{agent wins with non natural} \\\\\n",
    "1\\quad\\quad\\text{agent wins on natural(if natural is set to False)} \\\\\n",
    "1.5\\quad\\text{agent wins on natural (if natural is set to True)}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that an episode has ended and the reward, $R_{T}$, has been returned from the Blackjack environment, we set the reward at time step $t$ of the episode to $R_{T}$<br>\n",
    "$$\\large R_{t} = R_{T}\\,\\,\\forall\\,\\,0\\le\\,\\,t\\,\\,\\lt\\,\\,T$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC Monte Carlo Prediction State Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm will use a returns function, $Returns(s_{t}, a_{t})$, to help us keep track of which states accumulate rewards $R_{T}$ at the end of each episode.<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large Returns(s_{t}) \\leftarrow\\text{list of accumulated returns }G\\text{ that resulted from being in state }s_{t}\\text{ over all episodes of a single run.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary R_s_a is a member of the [returnsStateActionBlackjack class](#returnsStateActionBlackjack.ipynb) and uses the key={((playerSum, dealerShows, usableAce), action)} : value={$Returns(s, a)$} and will be used later within the Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([(((0, 1, True), 0), []), (((0, 1, True), 1), []), (((0, 1, False), 0), []), (((0, 1, False), 1), []), (((0, 2, True), 0), []), (((0, 2, True), 1), []), (((0, 2, False), 0), []), (((0, 2, False), 1), []), (((0, 3, True), 0), []), (((0, 3, True), 1), []), (((0, 3, False), 0), []), (((0, 3, False), 1), []), (((0, 4, True), 0), []), (((0, 4, True), 1), []), (((0, 4, False), 0), []), (((0, 4, False), 1), []), (((0, 5, True), 0), []), (((0, 5, True), 1), []), (((0, 5, False), 0), []), (((0, 5, False), 1), []), (((0, 6, True), 0), []), (((0, 6, True), 1), []), (((0, 6, False), 0), []), (((0, 6, False), 1), []), (((0, 7, True), 0), []), (((0, 7, True), 1), []), (((0, 7, False), 0), []), (((0, 7, False), 1), []), (((0, 8, True), 0), []), (((0, 8, True), 1), []), (((0, 8, False), 0), []), (((0, 8, False), 1), []), (((0, 9, True), 0), []), (((0, 9, True), 1), []), (((0, 9, False), 0), []), (((0, 9, False), 1), []), (((0, 10, True), 0), []), (((0, 10, True), 1), []), (((0, 10, False), 0), []), (((0, 10, False), 1), []), (((1, 1, True), 0), []), (((1, 1, True), 1), []), (((1, 1, False), 0), []), (((1, 1, False), 1), []), (((1, 2, True), 0), []), (((1, 2, True), 1), []), (((1, 2, False), 0), []), (((1, 2, False), 1), []), (((1, 3, True), 0), []), (((1, 3, True), 1), []), (((1, 3, False), 0), []), (((1, 3, False), 1), []), (((1, 4, True), 0), []), (((1, 4, True), 1), []), (((1, 4, False), 0), []), (((1, 4, False), 1), []), (((1, 5, True), 0), []), (((1, 5, True), 1), []), (((1, 5, False), 0), []), (((1, 5, False), 1), []), (((1, 6, True), 0), []), (((1, 6, True), 1), []), (((1, 6, False), 0), []), (((1, 6, False), 1), []), (((1, 7, True), 0), []), (((1, 7, True), 1), []), (((1, 7, False), 0), []), (((1, 7, False), 1), []), (((1, 8, True), 0), []), (((1, 8, True), 1), []), (((1, 8, False), 0), []), (((1, 8, False), 1), []), (((1, 9, True), 0), []), (((1, 9, True), 1), []), (((1, 9, False), 0), []), (((1, 9, False), 1), []), (((1, 10, True), 0), []), (((1, 10, True), 1), []), (((1, 10, False), 0), []), (((1, 10, False), 1), []), (((2, 1, True), 0), []), (((2, 1, True), 1), []), (((2, 1, False), 0), []), (((2, 1, False), 1), []), (((2, 2, True), 0), []), (((2, 2, True), 1), []), (((2, 2, False), 0), []), (((2, 2, False), 1), []), (((2, 3, True), 0), []), (((2, 3, True), 1), []), (((2, 3, False), 0), []), (((2, 3, False), 1), []), (((2, 4, True), 0), []), (((2, 4, True), 1), []), (((2, 4, False), 0), []), (((2, 4, False), 1), []), (((2, 5, True), 0), []), (((2, 5, True), 1), []), (((2, 5, False), 0), []), (((2, 5, False), 1), []), (((2, 6, True), 0), []), (((2, 6, True), 1), []), (((2, 6, False), 0), []), (((2, 6, False), 1), []), (((2, 7, True), 0), []), (((2, 7, True), 1), []), (((2, 7, False), 0), []), (((2, 7, False), 1), []), (((2, 8, True), 0), []), (((2, 8, True), 1), []), (((2, 8, False), 0), []), (((2, 8, False), 1), []), (((2, 9, True), 0), []), (((2, 9, True), 1), []), (((2, 9, False), 0), []), (((2, 9, False), 1), []), (((2, 10, True), 0), []), (((2, 10, True), 1), []), (((2, 10, False), 0), []), (((2, 10, False), 1), []), (((3, 1, True), 0), []), (((3, 1, True), 1), []), (((3, 1, False), 0), []), (((3, 1, False), 1), []), (((3, 2, True), 0), []), (((3, 2, True), 1), []), (((3, 2, False), 0), []), (((3, 2, False), 1), []), (((3, 3, True), 0), []), (((3, 3, True), 1), []), (((3, 3, False), 0), []), (((3, 3, False), 1), []), (((3, 4, True), 0), []), (((3, 4, True), 1), []), (((3, 4, False), 0), []), (((3, 4, False), 1), []), (((3, 5, True), 0), []), (((3, 5, True), 1), []), (((3, 5, False), 0), []), (((3, 5, False), 1), []), (((3, 6, True), 0), []), (((3, 6, True), 1), []), (((3, 6, False), 0), []), (((3, 6, False), 1), []), (((3, 7, True), 0), []), (((3, 7, True), 1), []), (((3, 7, False), 0), []), (((3, 7, False), 1), []), (((3, 8, True), 0), []), (((3, 8, True), 1), []), (((3, 8, False), 0), []), (((3, 8, False), 1), []), (((3, 9, True), 0), []), (((3, 9, True), 1), []), (((3, 9, False), 0), []), (((3, 9, False), 1), []), (((3, 10, True), 0), []), (((3, 10, True), 1), []), (((3, 10, False), 0), []), (((3, 10, False), 1), []), (((4, 1, True), 0), []), (((4, 1, True), 1), []), (((4, 1, False), 0), []), (((4, 1, False), 1), []), (((4, 2, True), 0), []), (((4, 2, True), 1), []), (((4, 2, False), 0), []), (((4, 2, False), 1), []), (((4, 3, True), 0), []), (((4, 3, True), 1), []), (((4, 3, False), 0), []), (((4, 3, False), 1), []), (((4, 4, True), 0), []), (((4, 4, True), 1), []), (((4, 4, False), 0), []), (((4, 4, False), 1), []), (((4, 5, True), 0), []), (((4, 5, True), 1), []), (((4, 5, False), 0), []), (((4, 5, False), 1), []), (((4, 6, True), 0), []), (((4, 6, True), 1), []), (((4, 6, False), 0), []), (((4, 6, False), 1), []), (((4, 7, True), 0), []), (((4, 7, True), 1), []), (((4, 7, False), 0), []), (((4, 7, False), 1), []), (((4, 8, True), 0), []), (((4, 8, True), 1), []), (((4, 8, False), 0), []), (((4, 8, False), 1), []), (((4, 9, True), 0), []), (((4, 9, True), 1), []), (((4, 9, False), 0), []), (((4, 9, False), 1), []), (((4, 10, True), 0), []), (((4, 10, True), 1), []), (((4, 10, False), 0), []), (((4, 10, False), 1), []), (((5, 1, True), 0), []), (((5, 1, True), 1), []), (((5, 1, False), 0), []), (((5, 1, False), 1), []), (((5, 2, True), 0), []), (((5, 2, True), 1), []), (((5, 2, False), 0), []), (((5, 2, False), 1), []), (((5, 3, True), 0), []), (((5, 3, True), 1), []), (((5, 3, False), 0), []), (((5, 3, False), 1), []), (((5, 4, True), 0), []), (((5, 4, True), 1), []), (((5, 4, False), 0), []), (((5, 4, False), 1), []), (((5, 5, True), 0), []), (((5, 5, True), 1), []), (((5, 5, False), 0), []), (((5, 5, False), 1), []), (((5, 6, True), 0), []), (((5, 6, True), 1), []), (((5, 6, False), 0), []), (((5, 6, False), 1), []), (((5, 7, True), 0), []), (((5, 7, True), 1), []), (((5, 7, False), 0), []), (((5, 7, False), 1), []), (((5, 8, True), 0), []), (((5, 8, True), 1), []), (((5, 8, False), 0), []), (((5, 8, False), 1), []), (((5, 9, True), 0), []), (((5, 9, True), 1), []), (((5, 9, False), 0), []), (((5, 9, False), 1), []), (((5, 10, True), 0), []), (((5, 10, True), 1), []), (((5, 10, False), 0), []), (((5, 10, False), 1), []), (((6, 1, True), 0), []), (((6, 1, True), 1), []), (((6, 1, False), 0), []), (((6, 1, False), 1), []), (((6, 2, True), 0), []), (((6, 2, True), 1), []), (((6, 2, False), 0), []), (((6, 2, False), 1), []), (((6, 3, True), 0), []), (((6, 3, True), 1), []), (((6, 3, False), 0), []), (((6, 3, False), 1), []), (((6, 4, True), 0), []), (((6, 4, True), 1), []), (((6, 4, False), 0), []), (((6, 4, False), 1), []), (((6, 5, True), 0), []), (((6, 5, True), 1), []), (((6, 5, False), 0), []), (((6, 5, False), 1), []), (((6, 6, True), 0), []), (((6, 6, True), 1), []), (((6, 6, False), 0), []), (((6, 6, False), 1), []), (((6, 7, True), 0), []), (((6, 7, True), 1), []), (((6, 7, False), 0), []), (((6, 7, False), 1), []), (((6, 8, True), 0), []), (((6, 8, True), 1), []), (((6, 8, False), 0), []), (((6, 8, False), 1), []), (((6, 9, True), 0), []), (((6, 9, True), 1), []), (((6, 9, False), 0), []), (((6, 9, False), 1), []), (((6, 10, True), 0), []), (((6, 10, True), 1), []), (((6, 10, False), 0), []), (((6, 10, False), 1), []), (((7, 1, True), 0), []), (((7, 1, True), 1), []), (((7, 1, False), 0), []), (((7, 1, False), 1), []), (((7, 2, True), 0), []), (((7, 2, True), 1), []), (((7, 2, False), 0), []), (((7, 2, False), 1), []), (((7, 3, True), 0), []), (((7, 3, True), 1), []), (((7, 3, False), 0), []), (((7, 3, False), 1), []), (((7, 4, True), 0), []), (((7, 4, True), 1), []), (((7, 4, False), 0), []), (((7, 4, False), 1), []), (((7, 5, True), 0), []), (((7, 5, True), 1), []), (((7, 5, False), 0), []), (((7, 5, False), 1), []), (((7, 6, True), 0), []), (((7, 6, True), 1), []), (((7, 6, False), 0), []), (((7, 6, False), 1), []), (((7, 7, True), 0), []), (((7, 7, True), 1), []), (((7, 7, False), 0), []), (((7, 7, False), 1), []), (((7, 8, True), 0), []), (((7, 8, True), 1), []), (((7, 8, False), 0), []), (((7, 8, False), 1), []), (((7, 9, True), 0), []), (((7, 9, True), 1), []), (((7, 9, False), 0), []), (((7, 9, False), 1), []), (((7, 10, True), 0), []), (((7, 10, True), 1), []), (((7, 10, False), 0), []), (((7, 10, False), 1), []), (((8, 1, True), 0), []), (((8, 1, True), 1), []), (((8, 1, False), 0), []), (((8, 1, False), 1), []), (((8, 2, True), 0), []), (((8, 2, True), 1), []), (((8, 2, False), 0), []), (((8, 2, False), 1), []), (((8, 3, True), 0), []), (((8, 3, True), 1), []), (((8, 3, False), 0), []), (((8, 3, False), 1), []), (((8, 4, True), 0), []), (((8, 4, True), 1), []), (((8, 4, False), 0), []), (((8, 4, False), 1), []), (((8, 5, True), 0), []), (((8, 5, True), 1), []), (((8, 5, False), 0), []), (((8, 5, False), 1), []), (((8, 6, True), 0), []), (((8, 6, True), 1), []), (((8, 6, False), 0), []), (((8, 6, False), 1), []), (((8, 7, True), 0), []), (((8, 7, True), 1), []), (((8, 7, False), 0), []), (((8, 7, False), 1), []), (((8, 8, True), 0), []), (((8, 8, True), 1), []), (((8, 8, False), 0), []), (((8, 8, False), 1), []), (((8, 9, True), 0), []), (((8, 9, True), 1), []), (((8, 9, False), 0), []), (((8, 9, False), 1), []), (((8, 10, True), 0), []), (((8, 10, True), 1), []), (((8, 10, False), 0), []), (((8, 10, False), 1), []), (((9, 1, True), 0), []), (((9, 1, True), 1), []), (((9, 1, False), 0), []), (((9, 1, False), 1), []), (((9, 2, True), 0), []), (((9, 2, True), 1), []), (((9, 2, False), 0), []), (((9, 2, False), 1), []), (((9, 3, True), 0), []), (((9, 3, True), 1), []), (((9, 3, False), 0), []), (((9, 3, False), 1), []), (((9, 4, True), 0), []), (((9, 4, True), 1), []), (((9, 4, False), 0), []), (((9, 4, False), 1), []), (((9, 5, True), 0), []), (((9, 5, True), 1), []), (((9, 5, False), 0), []), (((9, 5, False), 1), []), (((9, 6, True), 0), []), (((9, 6, True), 1), []), (((9, 6, False), 0), []), (((9, 6, False), 1), []), (((9, 7, True), 0), []), (((9, 7, True), 1), []), (((9, 7, False), 0), []), (((9, 7, False), 1), []), (((9, 8, True), 0), []), (((9, 8, True), 1), []), (((9, 8, False), 0), []), (((9, 8, False), 1), []), (((9, 9, True), 0), []), (((9, 9, True), 1), []), (((9, 9, False), 0), []), (((9, 9, False), 1), []), (((9, 10, True), 0), []), (((9, 10, True), 1), []), (((9, 10, False), 0), []), (((9, 10, False), 1), []), (((10, 1, True), 0), []), (((10, 1, True), 1), []), (((10, 1, False), 0), []), (((10, 1, False), 1), []), (((10, 2, True), 0), []), (((10, 2, True), 1), []), (((10, 2, False), 0), []), (((10, 2, False), 1), []), (((10, 3, True), 0), []), (((10, 3, True), 1), []), (((10, 3, False), 0), []), (((10, 3, False), 1), []), (((10, 4, True), 0), []), (((10, 4, True), 1), []), (((10, 4, False), 0), []), (((10, 4, False), 1), []), (((10, 5, True), 0), []), (((10, 5, True), 1), []), (((10, 5, False), 0), []), (((10, 5, False), 1), []), (((10, 6, True), 0), []), (((10, 6, True), 1), []), (((10, 6, False), 0), []), (((10, 6, False), 1), []), (((10, 7, True), 0), []), (((10, 7, True), 1), []), (((10, 7, False), 0), []), (((10, 7, False), 1), []), (((10, 8, True), 0), []), (((10, 8, True), 1), []), (((10, 8, False), 0), []), (((10, 8, False), 1), []), (((10, 9, True), 0), []), (((10, 9, True), 1), []), (((10, 9, False), 0), []), (((10, 9, False), 1), []), (((10, 10, True), 0), []), (((10, 10, True), 1), []), (((10, 10, False), 0), []), (((10, 10, False), 1), []), (((11, 1, True), 0), []), (((11, 1, True), 1), []), (((11, 1, False), 0), []), (((11, 1, False), 1), []), (((11, 2, True), 0), []), (((11, 2, True), 1), []), (((11, 2, False), 0), []), (((11, 2, False), 1), []), (((11, 3, True), 0), []), (((11, 3, True), 1), []), (((11, 3, False), 0), []), (((11, 3, False), 1), []), (((11, 4, True), 0), []), (((11, 4, True), 1), []), (((11, 4, False), 0), []), (((11, 4, False), 1), []), (((11, 5, True), 0), []), (((11, 5, True), 1), []), (((11, 5, False), 0), []), (((11, 5, False), 1), []), (((11, 6, True), 0), []), (((11, 6, True), 1), []), (((11, 6, False), 0), []), (((11, 6, False), 1), []), (((11, 7, True), 0), []), (((11, 7, True), 1), []), (((11, 7, False), 0), []), (((11, 7, False), 1), []), (((11, 8, True), 0), []), (((11, 8, True), 1), []), (((11, 8, False), 0), []), (((11, 8, False), 1), []), (((11, 9, True), 0), []), (((11, 9, True), 1), []), (((11, 9, False), 0), []), (((11, 9, False), 1), []), (((11, 10, True), 0), []), (((11, 10, True), 1), []), (((11, 10, False), 0), []), (((11, 10, False), 1), []), (((12, 1, True), 0), []), (((12, 1, True), 1), []), (((12, 1, False), 0), []), (((12, 1, False), 1), []), (((12, 2, True), 0), []), (((12, 2, True), 1), []), (((12, 2, False), 0), []), (((12, 2, False), 1), []), (((12, 3, True), 0), []), (((12, 3, True), 1), []), (((12, 3, False), 0), []), (((12, 3, False), 1), []), (((12, 4, True), 0), []), (((12, 4, True), 1), []), (((12, 4, False), 0), []), (((12, 4, False), 1), []), (((12, 5, True), 0), []), (((12, 5, True), 1), []), (((12, 5, False), 0), []), (((12, 5, False), 1), []), (((12, 6, True), 0), []), (((12, 6, True), 1), []), (((12, 6, False), 0), []), (((12, 6, False), 1), []), (((12, 7, True), 0), []), (((12, 7, True), 1), []), (((12, 7, False), 0), []), (((12, 7, False), 1), []), (((12, 8, True), 0), []), (((12, 8, True), 1), []), (((12, 8, False), 0), []), (((12, 8, False), 1), []), (((12, 9, True), 0), []), (((12, 9, True), 1), []), (((12, 9, False), 0), []), (((12, 9, False), 1), []), (((12, 10, True), 0), []), (((12, 10, True), 1), []), (((12, 10, False), 0), []), (((12, 10, False), 1), []), (((13, 1, True), 0), []), (((13, 1, True), 1), []), (((13, 1, False), 0), []), (((13, 1, False), 1), []), (((13, 2, True), 0), []), (((13, 2, True), 1), []), (((13, 2, False), 0), []), (((13, 2, False), 1), []), (((13, 3, True), 0), []), (((13, 3, True), 1), []), (((13, 3, False), 0), []), (((13, 3, False), 1), []), (((13, 4, True), 0), []), (((13, 4, True), 1), []), (((13, 4, False), 0), []), (((13, 4, False), 1), []), (((13, 5, True), 0), []), (((13, 5, True), 1), []), (((13, 5, False), 0), []), (((13, 5, False), 1), []), (((13, 6, True), 0), []), (((13, 6, True), 1), []), (((13, 6, False), 0), []), (((13, 6, False), 1), []), (((13, 7, True), 0), []), (((13, 7, True), 1), []), (((13, 7, False), 0), []), (((13, 7, False), 1), []), (((13, 8, True), 0), []), (((13, 8, True), 1), []), (((13, 8, False), 0), []), (((13, 8, False), 1), []), (((13, 9, True), 0), []), (((13, 9, True), 1), []), (((13, 9, False), 0), []), (((13, 9, False), 1), []), (((13, 10, True), 0), []), (((13, 10, True), 1), []), (((13, 10, False), 0), []), (((13, 10, False), 1), []), (((14, 1, True), 0), []), (((14, 1, True), 1), []), (((14, 1, False), 0), []), (((14, 1, False), 1), []), (((14, 2, True), 0), []), (((14, 2, True), 1), []), (((14, 2, False), 0), []), (((14, 2, False), 1), []), (((14, 3, True), 0), []), (((14, 3, True), 1), []), (((14, 3, False), 0), []), (((14, 3, False), 1), []), (((14, 4, True), 0), []), (((14, 4, True), 1), []), (((14, 4, False), 0), []), (((14, 4, False), 1), []), (((14, 5, True), 0), []), (((14, 5, True), 1), []), (((14, 5, False), 0), []), (((14, 5, False), 1), []), (((14, 6, True), 0), []), (((14, 6, True), 1), []), (((14, 6, False), 0), []), (((14, 6, False), 1), []), (((14, 7, True), 0), []), (((14, 7, True), 1), []), (((14, 7, False), 0), []), (((14, 7, False), 1), []), (((14, 8, True), 0), []), (((14, 8, True), 1), []), (((14, 8, False), 0), []), (((14, 8, False), 1), []), (((14, 9, True), 0), []), (((14, 9, True), 1), []), (((14, 9, False), 0), []), (((14, 9, False), 1), []), (((14, 10, True), 0), []), (((14, 10, True), 1), []), (((14, 10, False), 0), []), (((14, 10, False), 1), []), (((15, 1, True), 0), []), (((15, 1, True), 1), []), (((15, 1, False), 0), []), (((15, 1, False), 1), []), (((15, 2, True), 0), []), (((15, 2, True), 1), []), (((15, 2, False), 0), []), (((15, 2, False), 1), []), (((15, 3, True), 0), []), (((15, 3, True), 1), []), (((15, 3, False), 0), []), (((15, 3, False), 1), []), (((15, 4, True), 0), []), (((15, 4, True), 1), []), (((15, 4, False), 0), []), (((15, 4, False), 1), []), (((15, 5, True), 0), []), (((15, 5, True), 1), []), (((15, 5, False), 0), []), (((15, 5, False), 1), []), (((15, 6, True), 0), []), (((15, 6, True), 1), []), (((15, 6, False), 0), []), (((15, 6, False), 1), []), (((15, 7, True), 0), []), (((15, 7, True), 1), []), (((15, 7, False), 0), []), (((15, 7, False), 1), []), (((15, 8, True), 0), []), (((15, 8, True), 1), []), (((15, 8, False), 0), []), (((15, 8, False), 1), []), (((15, 9, True), 0), []), (((15, 9, True), 1), []), (((15, 9, False), 0), []), (((15, 9, False), 1), []), (((15, 10, True), 0), []), (((15, 10, True), 1), []), (((15, 10, False), 0), []), (((15, 10, False), 1), []), (((16, 1, True), 0), []), (((16, 1, True), 1), []), (((16, 1, False), 0), []), (((16, 1, False), 1), []), (((16, 2, True), 0), []), (((16, 2, True), 1), []), (((16, 2, False), 0), []), (((16, 2, False), 1), []), (((16, 3, True), 0), []), (((16, 3, True), 1), []), (((16, 3, False), 0), []), (((16, 3, False), 1), []), (((16, 4, True), 0), []), (((16, 4, True), 1), []), (((16, 4, False), 0), []), (((16, 4, False), 1), []), (((16, 5, True), 0), []), (((16, 5, True), 1), []), (((16, 5, False), 0), []), (((16, 5, False), 1), []), (((16, 6, True), 0), []), (((16, 6, True), 1), []), (((16, 6, False), 0), []), (((16, 6, False), 1), []), (((16, 7, True), 0), []), (((16, 7, True), 1), []), (((16, 7, False), 0), []), (((16, 7, False), 1), []), (((16, 8, True), 0), []), (((16, 8, True), 1), []), (((16, 8, False), 0), []), (((16, 8, False), 1), []), (((16, 9, True), 0), []), (((16, 9, True), 1), []), (((16, 9, False), 0), []), (((16, 9, False), 1), []), (((16, 10, True), 0), []), (((16, 10, True), 1), []), (((16, 10, False), 0), []), (((16, 10, False), 1), []), (((17, 1, True), 0), []), (((17, 1, True), 1), []), (((17, 1, False), 0), []), (((17, 1, False), 1), []), (((17, 2, True), 0), []), (((17, 2, True), 1), []), (((17, 2, False), 0), []), (((17, 2, False), 1), []), (((17, 3, True), 0), []), (((17, 3, True), 1), []), (((17, 3, False), 0), []), (((17, 3, False), 1), []), (((17, 4, True), 0), []), (((17, 4, True), 1), []), (((17, 4, False), 0), []), (((17, 4, False), 1), []), (((17, 5, True), 0), []), (((17, 5, True), 1), []), (((17, 5, False), 0), []), (((17, 5, False), 1), []), (((17, 6, True), 0), []), (((17, 6, True), 1), []), (((17, 6, False), 0), []), (((17, 6, False), 1), []), (((17, 7, True), 0), []), (((17, 7, True), 1), []), (((17, 7, False), 0), []), (((17, 7, False), 1), []), (((17, 8, True), 0), []), (((17, 8, True), 1), []), (((17, 8, False), 0), []), (((17, 8, False), 1), []), (((17, 9, True), 0), []), (((17, 9, True), 1), []), (((17, 9, False), 0), []), (((17, 9, False), 1), []), (((17, 10, True), 0), []), (((17, 10, True), 1), []), (((17, 10, False), 0), []), (((17, 10, False), 1), []), (((18, 1, True), 0), []), (((18, 1, True), 1), []), (((18, 1, False), 0), []), (((18, 1, False), 1), []), (((18, 2, True), 0), []), (((18, 2, True), 1), []), (((18, 2, False), 0), []), (((18, 2, False), 1), []), (((18, 3, True), 0), []), (((18, 3, True), 1), []), (((18, 3, False), 0), []), (((18, 3, False), 1), []), (((18, 4, True), 0), []), (((18, 4, True), 1), []), (((18, 4, False), 0), []), (((18, 4, False), 1), []), (((18, 5, True), 0), []), (((18, 5, True), 1), []), (((18, 5, False), 0), []), (((18, 5, False), 1), []), (((18, 6, True), 0), []), (((18, 6, True), 1), []), (((18, 6, False), 0), []), (((18, 6, False), 1), []), (((18, 7, True), 0), []), (((18, 7, True), 1), []), (((18, 7, False), 0), []), (((18, 7, False), 1), []), (((18, 8, True), 0), []), (((18, 8, True), 1), []), (((18, 8, False), 0), []), (((18, 8, False), 1), []), (((18, 9, True), 0), []), (((18, 9, True), 1), []), (((18, 9, False), 0), []), (((18, 9, False), 1), []), (((18, 10, True), 0), []), (((18, 10, True), 1), []), (((18, 10, False), 0), []), (((18, 10, False), 1), []), (((19, 1, True), 0), []), (((19, 1, True), 1), []), (((19, 1, False), 0), []), (((19, 1, False), 1), []), (((19, 2, True), 0), []), (((19, 2, True), 1), []), (((19, 2, False), 0), []), (((19, 2, False), 1), []), (((19, 3, True), 0), []), (((19, 3, True), 1), []), (((19, 3, False), 0), []), (((19, 3, False), 1), []), (((19, 4, True), 0), []), (((19, 4, True), 1), []), (((19, 4, False), 0), []), (((19, 4, False), 1), []), (((19, 5, True), 0), []), (((19, 5, True), 1), []), (((19, 5, False), 0), []), (((19, 5, False), 1), []), (((19, 6, True), 0), []), (((19, 6, True), 1), []), (((19, 6, False), 0), []), (((19, 6, False), 1), []), (((19, 7, True), 0), []), (((19, 7, True), 1), []), (((19, 7, False), 0), []), (((19, 7, False), 1), []), (((19, 8, True), 0), []), (((19, 8, True), 1), []), (((19, 8, False), 0), []), (((19, 8, False), 1), []), (((19, 9, True), 0), []), (((19, 9, True), 1), []), (((19, 9, False), 0), []), (((19, 9, False), 1), []), (((19, 10, True), 0), []), (((19, 10, True), 1), []), (((19, 10, False), 0), []), (((19, 10, False), 1), []), (((20, 1, True), 0), []), (((20, 1, True), 1), []), (((20, 1, False), 0), []), (((20, 1, False), 1), []), (((20, 2, True), 0), []), (((20, 2, True), 1), []), (((20, 2, False), 0), []), (((20, 2, False), 1), []), (((20, 3, True), 0), []), (((20, 3, True), 1), []), (((20, 3, False), 0), []), (((20, 3, False), 1), []), (((20, 4, True), 0), []), (((20, 4, True), 1), []), (((20, 4, False), 0), []), (((20, 4, False), 1), []), (((20, 5, True), 0), []), (((20, 5, True), 1), []), (((20, 5, False), 0), []), (((20, 5, False), 1), []), (((20, 6, True), 0), []), (((20, 6, True), 1), []), (((20, 6, False), 0), []), (((20, 6, False), 1), []), (((20, 7, True), 0), []), (((20, 7, True), 1), []), (((20, 7, False), 0), []), (((20, 7, False), 1), []), (((20, 8, True), 0), []), (((20, 8, True), 1), []), (((20, 8, False), 0), []), (((20, 8, False), 1), []), (((20, 9, True), 0), []), (((20, 9, True), 1), []), (((20, 9, False), 0), []), (((20, 9, False), 1), []), (((20, 10, True), 0), []), (((20, 10, True), 1), []), (((20, 10, False), 0), []), (((20, 10, False), 1), []), (((21, 1, True), 0), []), (((21, 1, True), 1), []), (((21, 1, False), 0), []), (((21, 1, False), 1), []), (((21, 2, True), 0), []), (((21, 2, True), 1), []), (((21, 2, False), 0), []), (((21, 2, False), 1), []), (((21, 3, True), 0), []), (((21, 3, True), 1), []), (((21, 3, False), 0), []), (((21, 3, False), 1), []), (((21, 4, True), 0), []), (((21, 4, True), 1), []), (((21, 4, False), 0), []), (((21, 4, False), 1), []), (((21, 5, True), 0), []), (((21, 5, True), 1), []), (((21, 5, False), 0), []), (((21, 5, False), 1), []), (((21, 6, True), 0), []), (((21, 6, True), 1), []), (((21, 6, False), 0), []), (((21, 6, False), 1), []), (((21, 7, True), 0), []), (((21, 7, True), 1), []), (((21, 7, False), 0), []), (((21, 7, False), 1), []), (((21, 8, True), 0), []), (((21, 8, True), 1), []), (((21, 8, False), 0), []), (((21, 8, False), 1), []), (((21, 9, True), 0), []), (((21, 9, True), 1), []), (((21, 9, False), 0), []), (((21, 9, False), 1), []), (((21, 10, True), 0), []), (((21, 10, True), 1), []), (((21, 10, False), 0), []), (((21, 10, False), 1), []), (((22, 1, True), 0), []), (((22, 1, True), 1), []), (((22, 1, False), 0), []), (((22, 1, False), 1), []), (((22, 2, True), 0), []), (((22, 2, True), 1), []), (((22, 2, False), 0), []), (((22, 2, False), 1), []), (((22, 3, True), 0), []), (((22, 3, True), 1), []), (((22, 3, False), 0), []), (((22, 3, False), 1), []), (((22, 4, True), 0), []), (((22, 4, True), 1), []), (((22, 4, False), 0), []), (((22, 4, False), 1), []), (((22, 5, True), 0), []), (((22, 5, True), 1), []), (((22, 5, False), 0), []), (((22, 5, False), 1), []), (((22, 6, True), 0), []), (((22, 6, True), 1), []), (((22, 6, False), 0), []), (((22, 6, False), 1), []), (((22, 7, True), 0), []), (((22, 7, True), 1), []), (((22, 7, False), 0), []), (((22, 7, False), 1), []), (((22, 8, True), 0), []), (((22, 8, True), 1), []), (((22, 8, False), 0), []), (((22, 8, False), 1), []), (((22, 9, True), 0), []), (((22, 9, True), 1), []), (((22, 9, False), 0), []), (((22, 9, False), 1), []), (((22, 10, True), 0), []), (((22, 10, True), 1), []), (((22, 10, False), 0), []), (((22, 10, False), 1), []), (((23, 1, True), 0), []), (((23, 1, True), 1), []), (((23, 1, False), 0), []), (((23, 1, False), 1), []), (((23, 2, True), 0), []), (((23, 2, True), 1), []), (((23, 2, False), 0), []), (((23, 2, False), 1), []), (((23, 3, True), 0), []), (((23, 3, True), 1), []), (((23, 3, False), 0), []), (((23, 3, False), 1), []), (((23, 4, True), 0), []), (((23, 4, True), 1), []), (((23, 4, False), 0), []), (((23, 4, False), 1), []), (((23, 5, True), 0), []), (((23, 5, True), 1), []), (((23, 5, False), 0), []), (((23, 5, False), 1), []), (((23, 6, True), 0), []), (((23, 6, True), 1), []), (((23, 6, False), 0), []), (((23, 6, False), 1), []), (((23, 7, True), 0), []), (((23, 7, True), 1), []), (((23, 7, False), 0), []), (((23, 7, False), 1), []), (((23, 8, True), 0), []), (((23, 8, True), 1), []), (((23, 8, False), 0), []), (((23, 8, False), 1), []), (((23, 9, True), 0), []), (((23, 9, True), 1), []), (((23, 9, False), 0), []), (((23, 9, False), 1), []), (((23, 10, True), 0), []), (((23, 10, True), 1), []), (((23, 10, False), 0), []), (((23, 10, False), 1), []), (((24, 1, True), 0), []), (((24, 1, True), 1), []), (((24, 1, False), 0), []), (((24, 1, False), 1), []), (((24, 2, True), 0), []), (((24, 2, True), 1), []), (((24, 2, False), 0), []), (((24, 2, False), 1), []), (((24, 3, True), 0), []), (((24, 3, True), 1), []), (((24, 3, False), 0), []), (((24, 3, False), 1), []), (((24, 4, True), 0), []), (((24, 4, True), 1), []), (((24, 4, False), 0), []), (((24, 4, False), 1), []), (((24, 5, True), 0), []), (((24, 5, True), 1), []), (((24, 5, False), 0), []), (((24, 5, False), 1), []), (((24, 6, True), 0), []), (((24, 6, True), 1), []), (((24, 6, False), 0), []), (((24, 6, False), 1), []), (((24, 7, True), 0), []), (((24, 7, True), 1), []), (((24, 7, False), 0), []), (((24, 7, False), 1), []), (((24, 8, True), 0), []), (((24, 8, True), 1), []), (((24, 8, False), 0), []), (((24, 8, False), 1), []), (((24, 9, True), 0), []), (((24, 9, True), 1), []), (((24, 9, False), 0), []), (((24, 9, False), 1), []), (((24, 10, True), 0), []), (((24, 10, True), 1), []), (((24, 10, False), 0), []), (((24, 10, False), 1), []), (((25, 1, True), 0), []), (((25, 1, True), 1), []), (((25, 1, False), 0), []), (((25, 1, False), 1), []), (((25, 2, True), 0), []), (((25, 2, True), 1), []), (((25, 2, False), 0), []), (((25, 2, False), 1), []), (((25, 3, True), 0), []), (((25, 3, True), 1), []), (((25, 3, False), 0), []), (((25, 3, False), 1), []), (((25, 4, True), 0), []), (((25, 4, True), 1), []), (((25, 4, False), 0), []), (((25, 4, False), 1), []), (((25, 5, True), 0), []), (((25, 5, True), 1), []), (((25, 5, False), 0), []), (((25, 5, False), 1), []), (((25, 6, True), 0), []), (((25, 6, True), 1), []), (((25, 6, False), 0), []), (((25, 6, False), 1), []), (((25, 7, True), 0), []), (((25, 7, True), 1), []), (((25, 7, False), 0), []), (((25, 7, False), 1), []), (((25, 8, True), 0), []), (((25, 8, True), 1), []), (((25, 8, False), 0), []), (((25, 8, False), 1), []), (((25, 9, True), 0), []), (((25, 9, True), 1), []), (((25, 9, False), 0), []), (((25, 9, False), 1), []), (((25, 10, True), 0), []), (((25, 10, True), 1), []), (((25, 10, False), 0), []), (((25, 10, False), 1), []), (((26, 1, True), 0), []), (((26, 1, True), 1), []), (((26, 1, False), 0), []), (((26, 1, False), 1), []), (((26, 2, True), 0), []), (((26, 2, True), 1), []), (((26, 2, False), 0), []), (((26, 2, False), 1), []), (((26, 3, True), 0), []), (((26, 3, True), 1), []), (((26, 3, False), 0), []), (((26, 3, False), 1), []), (((26, 4, True), 0), []), (((26, 4, True), 1), []), (((26, 4, False), 0), []), (((26, 4, False), 1), []), (((26, 5, True), 0), []), (((26, 5, True), 1), []), (((26, 5, False), 0), []), (((26, 5, False), 1), []), (((26, 6, True), 0), []), (((26, 6, True), 1), []), (((26, 6, False), 0), []), (((26, 6, False), 1), []), (((26, 7, True), 0), []), (((26, 7, True), 1), []), (((26, 7, False), 0), []), (((26, 7, False), 1), []), (((26, 8, True), 0), []), (((26, 8, True), 1), []), (((26, 8, False), 0), []), (((26, 8, False), 1), []), (((26, 9, True), 0), []), (((26, 9, True), 1), []), (((26, 9, False), 0), []), (((26, 9, False), 1), []), (((26, 10, True), 0), []), (((26, 10, True), 1), []), (((26, 10, False), 0), []), (((26, 10, False), 1), []), (((27, 1, True), 0), []), (((27, 1, True), 1), []), (((27, 1, False), 0), []), (((27, 1, False), 1), []), (((27, 2, True), 0), []), (((27, 2, True), 1), []), (((27, 2, False), 0), []), (((27, 2, False), 1), []), (((27, 3, True), 0), []), (((27, 3, True), 1), []), (((27, 3, False), 0), []), (((27, 3, False), 1), []), (((27, 4, True), 0), []), (((27, 4, True), 1), []), (((27, 4, False), 0), []), (((27, 4, False), 1), []), (((27, 5, True), 0), []), (((27, 5, True), 1), []), (((27, 5, False), 0), []), (((27, 5, False), 1), []), (((27, 6, True), 0), []), (((27, 6, True), 1), []), (((27, 6, False), 0), []), (((27, 6, False), 1), []), (((27, 7, True), 0), []), (((27, 7, True), 1), []), (((27, 7, False), 0), []), (((27, 7, False), 1), []), (((27, 8, True), 0), []), (((27, 8, True), 1), []), (((27, 8, False), 0), []), (((27, 8, False), 1), []), (((27, 9, True), 0), []), (((27, 9, True), 1), []), (((27, 9, False), 0), []), (((27, 9, False), 1), []), (((27, 10, True), 0), []), (((27, 10, True), 1), []), (((27, 10, False), 0), []), (((27, 10, False), 1), []), (((28, 1, True), 0), []), (((28, 1, True), 1), []), (((28, 1, False), 0), []), (((28, 1, False), 1), []), (((28, 2, True), 0), []), (((28, 2, True), 1), []), (((28, 2, False), 0), []), (((28, 2, False), 1), []), (((28, 3, True), 0), []), (((28, 3, True), 1), []), (((28, 3, False), 0), []), (((28, 3, False), 1), []), (((28, 4, True), 0), []), (((28, 4, True), 1), []), (((28, 4, False), 0), []), (((28, 4, False), 1), []), (((28, 5, True), 0), []), (((28, 5, True), 1), []), (((28, 5, False), 0), []), (((28, 5, False), 1), []), (((28, 6, True), 0), []), (((28, 6, True), 1), []), (((28, 6, False), 0), []), (((28, 6, False), 1), []), (((28, 7, True), 0), []), (((28, 7, True), 1), []), (((28, 7, False), 0), []), (((28, 7, False), 1), []), (((28, 8, True), 0), []), (((28, 8, True), 1), []), (((28, 8, False), 0), []), (((28, 8, False), 1), []), (((28, 9, True), 0), []), (((28, 9, True), 1), []), (((28, 9, False), 0), []), (((28, 9, False), 1), []), (((28, 10, True), 0), []), (((28, 10, True), 1), []), (((28, 10, False), 0), []), (((28, 10, False), 1), []), (((29, 1, True), 0), []), (((29, 1, True), 1), []), (((29, 1, False), 0), []), (((29, 1, False), 1), []), (((29, 2, True), 0), []), (((29, 2, True), 1), []), (((29, 2, False), 0), []), (((29, 2, False), 1), []), (((29, 3, True), 0), []), (((29, 3, True), 1), []), (((29, 3, False), 0), []), (((29, 3, False), 1), []), (((29, 4, True), 0), []), (((29, 4, True), 1), []), (((29, 4, False), 0), []), (((29, 4, False), 1), []), (((29, 5, True), 0), []), (((29, 5, True), 1), []), (((29, 5, False), 0), []), (((29, 5, False), 1), []), (((29, 6, True), 0), []), (((29, 6, True), 1), []), (((29, 6, False), 0), []), (((29, 6, False), 1), []), (((29, 7, True), 0), []), (((29, 7, True), 1), []), (((29, 7, False), 0), []), (((29, 7, False), 1), []), (((29, 8, True), 0), []), (((29, 8, True), 1), []), (((29, 8, False), 0), []), (((29, 8, False), 1), []), (((29, 9, True), 0), []), (((29, 9, True), 1), []), (((29, 9, False), 0), []), (((29, 9, False), 1), []), (((29, 10, True), 0), []), (((29, 10, True), 1), []), (((29, 10, False), 0), []), (((29, 10, False), 1), []), (((30, 1, True), 0), []), (((30, 1, True), 1), []), (((30, 1, False), 0), []), (((30, 1, False), 1), []), (((30, 2, True), 0), []), (((30, 2, True), 1), []), (((30, 2, False), 0), []), (((30, 2, False), 1), []), (((30, 3, True), 0), []), (((30, 3, True), 1), []), (((30, 3, False), 0), []), (((30, 3, False), 1), []), (((30, 4, True), 0), []), (((30, 4, True), 1), []), (((30, 4, False), 0), []), (((30, 4, False), 1), []), (((30, 5, True), 0), []), (((30, 5, True), 1), []), (((30, 5, False), 0), []), (((30, 5, False), 1), []), (((30, 6, True), 0), []), (((30, 6, True), 1), []), (((30, 6, False), 0), []), (((30, 6, False), 1), []), (((30, 7, True), 0), []), (((30, 7, True), 1), []), (((30, 7, False), 0), []), (((30, 7, False), 1), []), (((30, 8, True), 0), []), (((30, 8, True), 1), []), (((30, 8, False), 0), []), (((30, 8, False), 1), []), (((30, 9, True), 0), []), (((30, 9, True), 1), []), (((30, 9, False), 0), []), (((30, 9, False), 1), []), (((30, 10, True), 0), []), (((30, 10, True), 1), []), (((30, 10, False), 0), []), (((30, 10, False), 1), []), (((31, 1, True), 0), []), (((31, 1, True), 1), []), (((31, 1, False), 0), []), (((31, 1, False), 1), []), (((31, 2, True), 0), []), (((31, 2, True), 1), []), (((31, 2, False), 0), []), (((31, 2, False), 1), []), (((31, 3, True), 0), []), (((31, 3, True), 1), []), (((31, 3, False), 0), []), (((31, 3, False), 1), []), (((31, 4, True), 0), []), (((31, 4, True), 1), []), (((31, 4, False), 0), []), (((31, 4, False), 1), []), (((31, 5, True), 0), []), (((31, 5, True), 1), []), (((31, 5, False), 0), []), (((31, 5, False), 1), []), (((31, 6, True), 0), []), (((31, 6, True), 1), []), (((31, 6, False), 0), []), (((31, 6, False), 1), []), (((31, 7, True), 0), []), (((31, 7, True), 1), []), (((31, 7, False), 0), []), (((31, 7, False), 1), []), (((31, 8, True), 0), []), (((31, 8, True), 1), []), (((31, 8, False), 0), []), (((31, 8, False), 1), []), (((31, 9, True), 0), []), (((31, 9, True), 1), []), (((31, 9, False), 0), []), (((31, 9, False), 1), []), (((31, 10, True), 0), []), (((31, 10, True), 1), []), (((31, 10, False), 0), []), (((31, 10, False), 1), [])])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and initialize a new temporary returns function R(S_t, A_t) with empty lists\n",
    "rsaTemp = returnsSA.returnsStateActionBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "rsaTemp.R_s_a.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rsaTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Target Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent's target policy is denoted as $\\pi$ and is used to represent the conditional probability mass function $f_{A|S}$ of actions over the states:<br>\n",
    "$$\\large \\pi\\coloneqq f_{A|S}$$\n",
    "\n",
    "We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large \\pi(a|s)=f_{A|S}(a|s)=P(A=a|S=s)$$\n",
    "\n",
    "Recall from the defination of the Action Space $A$ from above, that $A$ follows a Bernoulli distribution where $P(A=a)$  We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large \\pi(a_{t}|s_{t})\n",
    "=\n",
    "\\begin{cases}\n",
    "0\\quad\\quad\\quad \\text{if } Q(s_{t}, a_{t}=0)\\lt Q(s_{t}, a_{t}=1) \\\\\n",
    "1\\quad\\quad\\quad \\text{if }Q(s_{t}, a_{t}=0)\\gt Q(s_{t}, a_{t}=1)\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary pi is a member of the [targetPolicyBlackjack class](#targetPolicyBlackjack.ipynb) and uses the key={((playerSum, dealerShows, usableAce), action)} : value={$\\pi(a_{t}\\,\\,|\\,\\,s_{t})$} pairs and will be used later within the Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Importance Sampling First Visit algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a temporary policy pi(s)\n",
    "piTemp = tPolicy.targetPolicyBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "piTemp.pi.items()\n",
    "del piTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Behavior Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent's behavior policy is denoted as $b$ and is used to represent the conditional probability mass function $f_{A|S}$ of actions over the states:<br>\n",
    "$$\\large b\\coloneqq f_{A|S}$$\n",
    "\n",
    "Our agent must, at timestep $t$, choose an action $a\\in A$ (either '0' to 'stick' or '1' to 'hit') to move the from its current state $s\\in S$ to its next state $s'\\in S$.\n",
    "\n",
    "We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large b(a|s)=f_{A|S}(a|s)=P(A=a|S=s)$$\n",
    "\n",
    "Recall from the defination of the Action Space $A$ from above, that $A$ follows a Bernoulli distribution where $P(A=a)$  We denote the conditional probability of our agent selecting action $a\\in A$ given its current state $s\\in S$ by the following notation:\n",
    "$$\\large b(a_{t}|s_{t})\n",
    "=\n",
    "\\begin{cases}\n",
    "0\\quad\\quad\\quad\\text{if }f(s_{t}, a_{t}=0)\\lt f(s_{t}, a_{t}=1) \\\\\n",
    "1\\quad\\quad\\quad\\text{if }f(s_{t}, a_{t}=0)\\ge f(s_{t}, a_{t}=1)\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a temporary policy b(s)\n",
    "bTemp = bPolicy.behaviorPolicyBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "bTemp.b.items()\n",
    "del bTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Action Value Function</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Bellman eqautions, the action value function $Q(s_{t}, a_{t})$ at time $t$ within an episode represents the expected return when choosing action $a_{t}\\in A$ when in state $s_{t}\\in S$ according to policy $\\pi_*(a_{t}|s_{t})$. [\\[4\\]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad q_\\pi(s_t, a_t)\\coloneqq\\mathbb{E} [G_t | S_t=s_t, A_t=a_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma G_{t+1} | S_t=s_t, A_t=a_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\\,\\,|\\,\\,S_{t}=s_{t},\\,\\,A_{t}=a_t]$<br>\n",
    "$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\sum\\limits_{s_{t+1}} \\sum\\limits_{r} P(s_{t+1}, r| s_{t}, a_{t})[r+\\gamma v_\\pi(s_{t+1})]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Prediction methods such as Off Policy Ordinary Per Decision Importance Sampling First Visit allow us to make an estimate $Q(s_{t}, a_{t})$ of the Action Value function $q_\\pi(s_{t}, a_{t})$ which will be demonstrated later and below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python dictionary Q_s_a is a member of the [stateActionValue class](#stateActionValueBlackjack.ipynb) and uses the key={((playerSum, dealerShows, usableAce), action)} : value={$Q(s_{t}, a_{t})$} pairs and will be used later within the Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"MonteCarloPrediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_FirstVisit_Blackjack\"></a>\n",
    "<h2>Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit Algorithm</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent must, at timestep $t$, choose an action $a\\in A$ (either '0' to 'stick' or '1' to 'hit') to move the from its current state $s\\in S$ to its next state $s'\\in S$ within a Blackjack episode. As in many reinforcement learning MDP problems, the the choice of action taken by an agent at each time step must either exploit the estimated optimal action $a_{t}\\in A$ given the current state $s_{t}\\in S$ or explore the action space for actions other than the optimal action in order to see if there are other actions with higher expected returns that may be better to choose going forward.\n",
    "As discussed in Barto's chapter on Monte Carlo methods for State Value function estimation using off policy mehtods [\\[5\\]](#References), the Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithim uses target policy $\\pi$ to exploit the optimal action, and a behavior policy $b$ to explore the action space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off policy algorithms learn from data that is not a result of the target policy $\\pi$ but rather some behavior policy $b$. As discussed in [\\[5\\]](#References). This notebook will demonstrate the problem of estimating the Action Value function $Q(S, A)\\approx q_{\\pi}(S, A)$ by using a Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm that uses fixed-given target and behavior policies $\\pi$ and $b$ respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the problem of estimating the action value function $Q(S, A)\\approx q_{\\pi}(S, A)$ by using an Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm that uses fixed-given target and behavior policies $\\pi$ and $b$ respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large q_{\\pi}(s_{t}, a_{t})\\,\\,=\\,\\,\\mathbb{E_{b}} [\\rho_{t:T-1}G_{t}|S_{t}=s_{t}, A_{t}=a_{t}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\rho$ transforms the returns while following the behavior policy $b$ as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of taking a trajectory according to the target policy $\\pi$ is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large P(A_{t},\\,\\,S_{t+1},\\,\\,A_{t+1},\\,\\,\\dots,\\,\\,S_{T}\\,\\,|\\,\\,S_{t},\\,\\,A_{t:T-1}\\sim\\pi)$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad =\\pi(A_{t}|S_{t})\\,\\,P(S_{t+1}|S_{t},\\,\\,A_{t})\\,\\,\\pi(A_{t+1}|S_{t+}),\\,\\,\\cdots,\\,\\,P(S_{T}|S_{T-1},\\,\\,A_{T-1})$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\quad\\prod\\limits_{k=t}^{T-1} \\pi(A_{k}|S_{k})\\,\\,P(S_{k+1}|S_{k},\\,\\,A_{k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the probability of taking a trajectory according to the behavior policy $b$ is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large P(A_{t},\\,\\,S_{t+1},\\,\\,A_{t+1},\\,\\,\\dots,\\,\\,S_{T}\\,\\,|\\,\\,S_{t},\\,\\,A_{t:T-1}\\sim b)$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=b(A_{t}|S_{t})\\,\\,P(S_{t+1}|S_{t},\\,\\,A_{t})\\,\\,b(A_{t+1}|S_{t+}),\\,\\,\\cdots,\\,\\,P(S_{T}|S_{T-1},\\,\\,A_{T-1})$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\quad\\prod\\limits_{k=t}^{T-1} b(A_{k}|S_{k})\\,\\,P(S_{k+1}|S_{k},\\,\\,A_{k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the ratio of these two probalities up until some time horizon h, we obtain the Discount Aware Ordinary Per Decision Importance Sampling Ratio $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large\\rho_{t:h}=\\frac{\\prod\\limits_{k=t}^{h-1} \\pi(A_{k}|S_{k})P(S_{k+1}|S_{k},\\,\\,A_{k})}{\\prod\\limits_{k=t}^{h-1} b(A_{k}|S_{k})P(S_{k+1}|S_{k},\\,\\,A_{k})}\\,\\,=\\,\\,\\prod\\limits_{k=t}^{h-1}\\frac{\\pi(A_{k}|S_{k})}{b(A_{k}|S_{k})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large\\rho_{t}=W_{t}^{t+1}=\\frac{\\pi(A_{t}|\\,S_{t})}{b(A_{t}|\\,S_{t})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large W_{t}^{h}=\\prod\\limits_{k=t}^{h-1} W_{k}^{k+1}=\\prod\\limits_{k=t}^{h-1} \\rho_{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product of the Discount Aware Ordinary Per Decision Importance Sampling Ratio $\\rho$ is used to scale the returns which we then take the average of those scaled returns obtained by following behavior policy $b$. These averaged scaled returns for following policy $b$ provide an estimate $Q(S_{t}, A_{t})\\approx q_{\\pi}(S_{t}, A_{t})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns can be scaled by discount factor $\\gamma_{t}$ and weighted by $W_{t}^{h}$ which is the Per Decision Importance Sampling Ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\hat G_{t}=\\sum \\limits_{l=t+1}^{T(t)}W_{t}^{l}\\prod \\limits_{i=t+1}^{l-1}\\gamma_{i} R_{l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large Q(S_{t}, A_{t})=\\frac{\\sum\\limits_{t\\in\\tau(s)}\\hat G_{t}}{|\\tau(s)|}$$\n",
    "$$\\large\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\text{where }\\tau(s)\\text{ is the set of all time steps in which state }s\\text{ was visited.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the action Value function when following policy $\\pi$ while only seeing the outcomes of following policy $b$, the Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit algorithm assumes that every action taken under $\\pi$ during an MDP is also taken occasionally under the behavior policy $b$ with some non-zero porobability.[\\[5\\]](#References) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(a_{t}|s_{t})\\gt 0 \\rightarrow b(a_{t}|s_{t})\\gt0\\quad\\quad\\quad\\forall\\,\\in\\,\\,A\\,\\,\\text{and}\\,s\\in\\,\\,S$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode for the Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importnace Sampling First Visit algorithm which estimates the action value function $Q(s_{t}, A_{t})$ is found below and as described in the Barto et. al. [\\[6\\]](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\\quad\\quad\\text{Monte Carlo Prediction Action Value Off Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit Algorithm}$<br>\n",
    "\n",
    "$\\large\\quad\\quad\\text{Inputs:}\\,\\,\\text{target policy\\,\\,}\\pi,\\,\\,\\text{behavior policy }b,\\,\\,environment,\\,\\,firstVisit=True,\\,\\,numEpisodes$<br>\n",
    "$\\large\\quad\\quad\\text{ 1.}\\quad Q(s, a)\\leftarrow\\text{Randomly chosen state action values all state-action pairs }(s\\in S, a\\in A)$<br>\n",
    "$\\large\\quad\\quad\\text{ 2.}\\quad R(s, a)\\leftarrow\\text{Empty list of returns for each state action pair }(s\\in S, a\\in A)$<br>\n",
    "$\\large\\quad\\quad\\text{ 3.}\\quad timeStep\\leftarrow 0$<br>\n",
    "$\\large\\quad\\quad\\text{ 4.}\\quad T\\leftarrow \\text{ empty dictionary }$<br>\n",
    "$\\large\\quad\\quad\\text{ 5.}\\quad episodeStateAction\\leftarrow\\text{empty list }$<br>\n",
    "$\\large\\quad\\quad\\text{ 6.}\\quad discountFactors\\leftarrow\\text{list \\{1\\} }$<br>\n",
    "$\\large\\quad\\quad\\text{ 7.}\\quad\\text{Loop forever (for each episode)}$:<br>\n",
    "$\\large\\quad\\quad\\text{ 8.}\\quad\\quad\\text{Allow the environment to provide }S_{t=startTime}\\in S\\text{ and choose action }A_{t=startTime}\\in A(S_{t=startTime})\\text{ randomly from the conditional distribution }b$<br>\n",
    "$\\large\\quad\\quad\\text{ 9.}\\quad\\quad\\text{Generate an episode from }S_{t=startTime}, A_{t=startTime},\\text{ following }b\\colon S_{t=starTime}, A_{t=starTime}, R_{t+1},..., S_{T(t=startTime)-1}, A_{T(t=startTime)-1}, R_{T(t=startTime)}$<br>\n",
    "$\\large\\quad\\quad\\text{ 10.}\\quad\\quad\\,\\hat G\\leftarrow\\,0\\quad W_{t}^{l}\\leftarrow 1$<br>\n",
    "$\\large\\quad\\quad\\text{ 11.}\\quad\\quad\\text{Loop for each step of episode, }t = T-1,\\,T-2,\\,\\dots,\\,0$<br>\n",
    "$\\large\\quad\\quad\\text{ 12.}\\quad\\quad\\quad\\,\\,\\,\\hat G\\leftarrow\\,\\sum \\limits_{l=t+1}^{T(t)} W_{t}^{l} \\prod \\limits_{i=t+1}^{l-1}\\gamma_{i} R_{l}$<br>\n",
    "$\\large\\quad\\quad\\text{ 13.}\\quad\\quad\\quad\\text{ if(firstVisit==False or (firstVisit==True and pair (}s_{t}, a_{t}\\text{) does not appear in } S_{0}, A_{0}, S_{1}, A_{1},\\,\\dots,\\,S_{t-1},\\,A_{t-1}\\text{))}\\colon$<br>\n",
    "$\\large\\quad\\quad\\text{ 14.}\\quad\\quad\\quad\\quad\\text{Append }\\hat G\\text{ to } R(S_{t}, A_{t})$<br>\n",
    "$\\large\\quad\\quad\\text{ 15.}\\quad\\quad\\quad\\quad Q(S_{t}, A_{t})\\leftarrow\\text{average}(R(S_{t}, A_{t}))$<br>\n",
    "$\\large\\quad\\quad\\text{ 16.}\\quad\\text{Return } Q(S, A)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Don't forget that $R_{t+1}=R_{T}\\,\\,\\forall\\,0\\le\\,t\\lt\\,T$ when using the Blackjack env as we do in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloPrediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_Blackjack(targetPolicy, behaviorPolicy, env, Q_s_a, R_s_a, firstVisit=True, numEpisodes=1):\n",
    "    # 3. Set stimeStep to zero\n",
    "    timeStep = 0\n",
    "    # 4. Set T to empty dictionary\n",
    "    T = {}\n",
    "    # 5. Create a list to track the observations (i.e. states) and actions taken during an episode\n",
    "    episodeStateAction = []\n",
    "    # 6. Create a list to store discount factors as we generate episodes\n",
    "    discountFactors = [0.999]\n",
    "    reward = (float)('-inf')\n",
    "    # 7. Run numEpisodes episodes (instead of looping forever as described in the pseudocode above)\n",
    "    for _ in range(numEpisodes):\n",
    "        # Set natural flag to ensure an action is needed to be taken by the agent\n",
    "        natural = False\n",
    "        # 8. Reset the envirionment and generate a random intitial state S_0\n",
    "        observation, info = env.reset(seed=(int)(datetime.now().timestamp() * 1000000))\n",
    "        # 8. Choose an action from policy pi based upon the observation \n",
    "        action = 0 if ( behaviorPolicy.b[((observation), 0)] > behaviorPolicy.b[((observation), 1)] ) else 1            \n",
    "        startTime = timeStep\n",
    "        # Set the done flag to false\n",
    "        done = False\n",
    "        # 9. Generate an episode from S_0 and A_0 follwoing policy pi\n",
    "        while ( not done ):\n",
    "            # Push the observation (i.e. state) action pair to the episode as the key and add an empty list as the value\n",
    "            episodeStateAction.append((observation, action))\n",
    "            timeStep += 1\n",
    "            # Step to the next state by performing an action \n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            discountFactors.append(discountFactors[-1] * .999)\n",
    "            if ( terminated or truncated ):\n",
    "                done = True\n",
    "                T[startTime] = timeStep\n",
    "                reward_T = reward\n",
    "                break\n",
    "            else:\n",
    "            # Choose an action from policy pi based upon the observation \n",
    "                action = 0 if ( behaviorPolicy.b[((observation), 0)] > behaviorPolicy.b[((observation), 1)] ) else 1            \n",
    "    \n",
    "        # This number should be appropriate in order to maintain \n",
    "        # the episode sequence {S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T-1, A_T-1, R_T}\n",
    "        reward_T = reward\n",
    "        # 10. Set G_hat to zero\n",
    "        G_hat = 0.0\n",
    "        # 10. Set the partial per decision importance sampling ratio W_t:h\n",
    "        W_t_l = 1.0\n",
    "        # 11. Loop for each step of episode, t=T-1, T-2, ..., 0\n",
    "        for t in range(timeStep-1, startTime-1,  -1):\n",
    "            # 12. Compute G tilde for time t\n",
    "            sum_l = 0.0\n",
    "            for l in range(t+1, T[startTime]+1):\n",
    "                for j in range(t, l):\n",
    "                    W_t_l *= targetPolicy.pi[episodeStateAction[j]] / behaviorPolicy.b[episodeStateAction[j]]\n",
    "                prod_Gamma_i = 1.0 \n",
    "                for i in range(t+1, l):\n",
    "                    prod_Gamma_i *= discountFactors[i]\n",
    "                R_l = reward_T\n",
    "                sum_l += W_t_l * prod_Gamma_i * R_l\n",
    "            G_hat += sum_l\n",
    "            # 13. If (firstVisit is False) or (firstVisit is True and state-action pair is not in in S_0, A_0, S_1, A_1, ... S_t-1, A_t-1)  \n",
    "            if ( (firstVisit==False) or ((firstVisit == True) and (not (episodeStateAction[t] in episodeStateAction[:t]))) ):\n",
    "                # 14. Append G_tilde to returns R(S_t, A_t)\n",
    "                R_s_a.R_s_a[episodeStateAction[t]].append(G_hat)\n",
    "                # 15. Set Q(S_t, A_t) to the average of the Returns(S_t)\n",
    "                Q_s_a.Q_s_a[episodeStateAction[t]] = sum(R_s_a.R_s_a[episodeStateAction[t]]) / len(R_s_a.R_s_a[episodeStateAction[t]])\n",
    "    # 16. Return the estimate of q_pi(s, a)\n",
    "    return Q_s_a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of runs\n",
    "numRuns = 1\n",
    "\n",
    "#episodes = [1, 10, 50, 100, 200, 500, 1000, 3000, 5000, 10000]\n",
    "episodes = [5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "for i in range(len(episodes)):\n",
    "    # Run an experiment j times\n",
    "    for runNum in range(numRuns):\n",
    "        # Set the environment \n",
    "        env = gym.make('Blackjack-v1', natural=False, sab=False)\n",
    "        # Use the RecorEpisodeStatistics wrapper to track rewards and episode lengths\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=episodes[i])\n",
    "        # Open a pre-trained target policy that was serialized to json in [\\[for use as an example\n",
    "        with open('../../MonteCarloControl/MonteCarloControl_OnPolicy_ExploringStarts_FirstVisit_Blackjack/results/target_policy.pickle', 'rb') as handle:\n",
    "            pi = pickle.load(handle)\n",
    "        # Initialize policy b(s) to equal probabilities 0.5 for all s in S\n",
    "        b = bPolicy.behaviorPolicyBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "        # 1. Instantiate and initialize a new state value function Q(S_t, A_t)\n",
    "        Q_s_a = actionValue.stateActionValueBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "        # 2. Instantiate and initialize a new returns function R(S_t) with empty lists\n",
    "        R_s_a = returnsSA.returnsStateActionBlackjack(stateSpacePlayerSum=stateSpacePlayerSum, stateSpaceDealerShows=stateSpaceDealerShows, stateSpaceUsableAce=stateSpaceUsableAce, actionSpace=actionSpace)\n",
    "        # Compute the estimate of the action value function Q\n",
    "        actionValueResult = MonteCarloPrediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_Blackjack(targetPolicy=pi, behaviorPolicy=b, env=env, Q_s_a=Q_s_a, R_s_a=R_s_a, firstVisit=True, numEpisodes=episodes[i])\n",
    "        # Set up the grids for plots of action value and policy when usable Ace is available\n",
    "        value_grid = h.create_action_value_grid(Q_s_a=actionValueResult, usable_ace=True)\n",
    "        # Format a string for the title of the plot when there is a usable Ace available\n",
    "        title = \"MC Prediction Action Value\\nOffPolicy Discount Aware Ordinary Per Decision Importance Sampling First Visit\\n#Episodes=\" + str(episodes[i]) + \", \\u03B3 \" + \"\\nRun# \" + str(runNum+1) + \", usableAce=T\\n\"\n",
    "        fileName =\"results/MC_Prediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_FirstVisit_Blackjack_Episodes_\" + str(episodes[i]) + \"_Run_\" + str(runNum+1) + \"_usableAce_T.png\"\n",
    "        h.create_action_value_plot(value_grid=value_grid, title=title, fileName=fileName, numEpisodes=episodes[i], runNum=runNum, firstVisit=True, usableAce=True)\n",
    "        # Set up the grids for plots of action value and policy when usable Ace is not available\n",
    "        value_grid = h.create_action_value_grid(Q_s_a=actionValueResult, usable_ace=False)\n",
    "        # Format a string for the title of the plot when there is a usable Ace is not available\n",
    "        title = \"MC Prediction Action Value\\nOff Policy Discount Aware Ordinary Per Decision Importance Sampling First Visit\\n#Episodes=\" + str(episodes[i]) + \", \\u03B3 \" + \"\\nRun# \" + str(runNum+1) + \", usableAce=F\\n\"\n",
    "        fileName =\"results/MC_Prediction_ActionValue_OffPolicy_DiscountAware_Ordinary_PerDecision_ImportanceSampling_FirstVisit_Blackjack_Episodes_\" + str(episodes[i]) + \"_Run_\" + str(runNum+1) + \"_usableAce_F.png\"\n",
    "        h.create_action_value_plot(value_grid=value_grid, title=title, fileName=fileName, numEpisodes=episodes[i], runNum=runNum, firstVisit=True, usableAce=False)\n",
    "        # Close the environment to clean up resources used by the environment\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "<h2>References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 58. \n",
    "\n",
    "2. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 99.\n",
    "\n",
    "3. https://gymnasium.farama.org/environments/toy_text/blackjack/\n",
    "\n",
    "4. https://github.com/openai/gym/issues/1410\n",
    "\n",
    "5. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 103-106. \n",
    "\n",
    "6. White A., White M. \"Sample-based Learnning Methods: Week 1\", Sample Based Learning Methods, \"Reinforcement Learning\" Specialization, www.coursera.org/learn/sample-based-learngin-methods/home/week1.\n",
    "\n",
    "7. [Monte Carlo Control On Policy Exploring Starts First Visit algorithm](#../MonteCarloControl/MonteCarloControl_OnPolicy_ExploringStarts_FirstVisit_Blackjack/MonteCarloControl_OnPolicy_ExploringStarts_FirstVisit_Blackjack.ipynb)\n",
    "\n",
    "8. Sutton R.S., Barto A.G., \"Reinforcement Learning, An Introduction\", 2nd ed, MIT Press, Cambridge MA, 2018, p. 114-115. \n",
    "\n",
    "9. Sutton R.S., \"Chapter 5: Monte Carlo Methods\", slide 28 of course slides at https://www.stanford.edu/class/cme241/lecture_slides/rich_sutton_slides/9-10-MC.pdf. \n",
    "\n",
    "10. Manhoot A., \"Incremental Off-policy reinforcement Learning Algorithms\", thesis 2017, https://doi.org/10.7939/R3NG4H58D, p. 50-52. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_python_3_9_kernel",
   "language": "python",
   "name": "basic_python_3_9_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
